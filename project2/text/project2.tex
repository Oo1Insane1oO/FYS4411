\documentclass[a4paper, hidelinks, 10pt]{article}
\usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}
\usepackage{libertine}
\usepackage[libertine]{newtxmath}
% \usepackage{charter}
% \usepackage[expert, charter]{mathdesign}
\usepackage{listings}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{mathtools}
% \usepackage{amssymb}
\usepackage[margin=0.7in]{geometry}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{float}
\usepackage{caption}
\usepackage{scrextend}
\usepackage{ragged2e}
\usepackage{multirow}
\usepackage{array}
\usepackage{physics}
\usepackage{enumitem}
\usepackage{varwidth}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xifthen}
\usepackage{etoolbox}
\usepackage{xparse}

%specific reused text
\newcommand{\mdate}{\today}
\newcommand{\mtitle}{FYS4411}
\newcommand{\mauthor}{Alfred Alocias Mariadason}
\newcommand{\massignn}{Project 2}

\pagestyle{fancy}
\fancyhf{}
% \fancyhead[LO, RE]{\small\leftmark}
\lhead{\small{\mtitle}}
\chead{\small{\massignn}}
\rhead{\small{\thesection}}
% \lfoot{}
\cfoot{\thepage}
% \rfoot{}

\patchcmd{\thebibliography}{\section*}{\section}{}{}

%renew title numbering
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\thesection.\Alph{subsection}}

%center title and subtitle
\let\oldsection\section
\renewcommand{\section}[1]{\centering \oldsection{{#1}} \justifying}
\let\oldsubsection\subsection
\renewcommand{\subsection}[1]{\centering \oldsubsection{{#1}} \justifying}

%set counter for algorithm
\newcommand{\algorithmautorefname}{algorithm}

%title settings
% \renewcommand{\headrulewidth}{0pt}
\renewcommand{\sectionautorefname}{section}
\renewcommand{\subsectionautorefname}{section}
\renewcommand{\subsubsectionautorefname}{section}
\renewcommand{\equationautorefname}{equation}
\renewcommand{\figureautorefname}{figure}
\renewcommand{\tableautorefname}{table}
\captionsetup{compatibility=false}

\patchcmd{\smallmatrix}{\thickspace}{\kern1.3em}{}{}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.3,0.3,0.3}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
        backgroundcolor=\color{backcolour},
        commentstyle=\color{codegreen},
        keywordstyle=\color{magenta},
        numberstyle=\tiny\color{codegray},
        stringstyle=\color{codepurple},
        basicstyle=\footnotesize,
        breakatwhitespace=false,
        breaklines=true,
        captionpos=b,
        keepspaces=true,
        numbers=left, 
        numbersep=4pt, 
        showspaces=false, 
        showstringspaces=false,
        showtabs=true, 
        tabsize=2
}
\lstset{style=mystyle}

\hypersetup{
    allcolors=black
}
\urlstyle{same}

\newcommand{\onefigure}[4]{
    \begin{figure}[H]
        \centering
        \textbf{{#1}}\\
        \includegraphics[scale=0.65]{{#2}}
        \caption{{#3}}
        \label{fig:#4}
    \end{figure}
    \justifying
} %one figure {filename}{caption}
\newcommand{\twofigure}[7]{
    \begin{figure}[H]
        \centering
        \begin{subfigure}[b!]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{{#1}}
            \caption{{#2}}
            \label{subfig:#3}
        \end{subfigure}
        \begin{subfigure}[b!]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{{#4}}
            \caption{{#5}}
            \label{subfig:#6}
        \end{subfigure}
        \caption{#7}
        \justify
    \end{figure}
} %two figure one-line {title}{file1}{caption1}{file2}{caption2}

\newcommand{\prtl}{\mathrm{\partial}} %reduce length of partial (less to write)
\newcommand{\md}{\mathrm{d}} %straight d for differential
\NewDocumentCommand{\prd}{m O{} O{}}{\frac{\prtl^{#3}{#2}}{\prtl{#1}^{#3}}}
\NewDocumentCommand{\mprd}{m O{} O{}}{\frac{\md^{#3}{#2}}{\md{#1}^{#3}}}
\newcommand{\vsp}{\vspace{0.2cm}} %small vertical space
\newcommand{\txtit}[1]{\textit{{#1}}} %italic text
\newcommand{\blds}[1]{\boldsymbol{{#1}}} % better bold in mathmode (from amsmath)
\newcommand{\bigO}{\mathcal{O}} %nice big O
\newcommand{\me}{\mathrm{e}} %straight e for exp
\newcommand{\mRe}[1]{\mathrm{Re}\left({#1}\right)}%nice real
\newcommand{\munit}[1]{\;\ensuremath{\, \mathrm{#1}}} %straight units in math
\newcommand{\Rarr}{\Rightarrow} %reduce lenght of Rightarrow (less to write)
\newcommand{\rarr}{\rightarrow} %reduce lenght of rightarrow (less to write)
\newcommand{\ecp}[1]{\left< {#1} \right>} %expected value
\newcommand{\urw}{\uparrow} % up arrow
\newcommand{\drw}{\downarrow} % up arrow
\newcommand{\pt}[1]{\textbf{\txtit{#1}}\justify}
\newcommand{\infint}{\int\limits^{\infty}_{-\infty}}
\newcommand{\oinfint}{\int\limits^{\infty}_0}
\newcommand{\sint}{\int\limits^{2\pi}_0\int\limits^{\pi}_0\oinfint}
\newcommand{\arcsinh}[1]{\text{arcsinh}\left(#1\right)}
\newcommand{\I}{\scalebox{1.2}{$\mathds{1}$}}
\newcommand{\veps}{\varepsilon} %\varepsilon is to long :P
\newcommand{\suml}[2]{\sum\limits_{#1=1}^{#2}}
\newcommand{\prodl}[2]{\prod\limits_{#1}^{#2}}

\newcommand{\fij}[3]{#1\left(#2\rarr#3\right)}
\newcommand{\ufij}[3]{#1_{#2\rarr#3}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\infoTabs}[2]{
    \begin{tabular}{crl}
        \multicolumn{3}{c}{\textbf{$\blds{N=#1}$}} \\
        \multicolumn{1}{c}{$R$} & \multicolumn{1}{c}{$E_0[au]$} &
        \multicolumn{1}{c}{$I$} \\
        \hline
        \input{#2}
    \end{tabular}
}

\newcommand{\infoTables}[7]{
    \begin{table}[H]
        \centering
        \textbf{Energies with $\blds{\omega=#1}$} \\
        \hrule \vspace{1.5pt} \hrule \vspace{0.7pt}
        \infoTabs{2}{#2}
        \infoTabs{6}{#3}
        \infoTabs{12}{#4}
        \infoTabs{20}{#5}
        \hrule \vspace{1.5pt} \hrule \vspace{0.7pt}
        \caption{#6}
        \justifying
        \label{tab:#7}
    \end{table}
}

\makeatletter
% define a macro \Autoref to allow multiple references to be passed to \autoref
\newcommand\Autoref[1]{\@first@ref#1,@}
\def\@throw@dot#1.#2@{#1}% discard everything after the dot
\def\@set@refname#1{%    % set \@refname to autoefname+s using \getrefbykeydefault
    \edef\@tmp{\getrefbykeydefault{#1}{anchor}{}}%
    \def\@refname{\@nameuse{\expandafter\@throw@dot\@tmp.@autorefname}s}%
}
\def\@first@ref#1,#2{%
  \ifx#2@\autoref{#1}\let\@nextref\@gobble% only one ref, revert to normal \autoref
  \else%
    \@set@refname{#1}%  set \@refname to autoref name
    \@refname~\ref{#1}% add autoefname and first reference
    \let\@nextref\@next@ref% push processing to \@next@ref
  \fi%
  \@nextref#2%
}
\def\@next@ref#1,#2{%
   \ifx#2@ and~\ref{#1}\let\@nextref\@gobble% at end: print and+\ref and stop
   \else, \ref{#1}% print  ,+\ref and continue
   \fi%
   \@nextref#2%
}
\makeatother

\begin{document}
\thispagestyle{empty}
\begin{center} \vspace{1cm}
    \textbf{\Large{\mtitle\hspace{0.01pt} - Computational Physics II: Quantum
    Mechanical Systems}}\\ \vspace{0.5cm}
    \textbf{\large{\massignn\hspace{0.01pt} - Variatonal Monte Carlo
    Methods}}\\ \vspace{1cm}
    \textbf{\large{\mauthor}}\\ \vspace{0.5cm}
    \large{\url{https://www.github.com/Oo1Insane1oO/FYS4411}} \\ \vspace{0.5cm}
    \Large{\mdate}\\ \vfill
\end{center}

\clearpage
\setcounter{page}{1}

\begin{center}
    \textbf{Abstract} \\
\end{center}

\section{INTRODUCTION}
\label{sec:introduction}
    Using the Variational Monte Carlo, this project aims to find and analyze
    quantities such as the ground state energy and single-particle densities of
    quantum dots for so-called closed shell systems.

    We use the usual approach by estimating expectation value of the ground
    state energy with the variational principle and minimizing. The algorithm
    used for the Monte Carlo method is the well known Metropolis algorithm.

    The reason for using a Monte Carlo method for minimizing the trial ground
    state energy is because the expectation value would in general be a
    multi-dimensional integral depending on the number of particles and number
    of parameters involved in the total wave function. Such an integral is not
    adequately solved by traditional methods(i.e Gaussian-quadrature).

    The desired result is that the Metropolis algorithm with importance
    sampling yields a better result both from a computational point of view.
    That is it finds a good estimate for the ground state energy efficiently
    without wasting to much time on the configuration space. The wave function
    only has small values in this large space meaning a homogeneous
    distribution of calculation points would yield a poor result, a
    non-homogeneous approach(such as with the Metropolis algorithm) would then,
    hopefully, gives a better result.

\section{THEORY}
\label{sec:theory}
    >>INSERT DESCRIPTION<<

\subsection{HERMITE POLYNOMIALS}
\label{sub:hermite_polynomials}
    Hermite polynomials $H(x)$ are solutions to the differential equation
        \begin{equation}
            \frac{\md^2 H}{\md x^2} -2x\frac{\md H}{\md x} + \left(\lambda
            -1\right)H = 0
            \label{eq:hermitediffeq}
        \end{equation}
    The polynomials fulfill the orthogonality relation 
        \begin{equation}
            \infint \me^{-x^2}H^2_n\md x = 2^nn!\sqrt{\pi}
            \label{eq:hermiteOrth}
        \end{equation}
    with the recurrence relation
        \begin{equation}
            H_{n+1} = 2xH_n - 2nH_{n-1}
            \label{eq:hermiteReq}
        \end{equation}
    and standardized relation
        \begin{equation}
            H_n = (-1)^n\me^{x^2}\prd{x}[][n]\me^{-x^2}
            \label{eq:hermiteExp}
        \end{equation}
    From \Autoref{eq:hermiteExp} one can find an expression for the derivative
    of the Hermite polynomial as
        \begin{equation}
            \prd{x}[H_n][m] = 2^mm!\frac{n!}{(n-m)!}H_{n-m}
            \label{eq:hermiteDer}
        \end{equation}

\subsection{HARMONIC OSCILLATOR}
\label{sub:harmonic_oscillator}
\subsubsection{Cartesian Coordinates}
\label{ssub:Cartesian Coordinates}
    The harmonic oscillator system in $2$ dimensions and in natural units is
    given by the following Hamiltonian
        \begin{equation}
            \hat{H_0} = \frac{1}{2}\sum^N_{i=1}\left(-\nabla^2_i + \omega^2
            r^2_i\right)
            \label{eq:cartHarmOsc}
        \end{equation}
    The wave functions in this case is then:
        \begin{equation}
            \phi_{n_x,n_y}(x,y) =
            AH_{n_x}(\sqrt{\omega}x)H_{n_y}(\sqrt{\omega}y)
            \exp(-\frac{\omega}{2}(x^2+y^2))
            \label{eq:cartUarmOscWavef}
        \end{equation}
    where $H_n$ is a Hermite polynomial of order $n$ and $A$ is a normalization
    constant. The quantum numbers $n_x$ and $n_y$ go as $n_x,n_y=0,1,2\dots$.
    While $\omega$ is the oscillator frequency. \\
    The energies is 
        \begin{equation}
            E = \hbar\omega\left(n_x + n_y + 1\right)
            \label{eq:cartHarmOscE}
        \end{equation}

\subsection{FOURIER TRANSFORMATION}
\label{sub:fourier_transformation}
    Given an integrable function $f:\mathbb{R}\rarr\mathbb{C}$ the Fourier
    transform to variable $x$ is defined to be
        \begin{equation}
            \tilde{f}(k) = \frac{1}{\sqrt{2\pi}}\infint f(x)\me^{ixs} \md x
            \label{eq:FourierTransform}
        \end{equation}
    with $x,s\in\mathbb{R}$. \\
    The inverse transformation is given as
        \begin{equation}
            f(x) = \frac{1}{\sqrt{2\pi}}\infint \tilde{f}(k)\me^{-isx} \md s
            \label{eq:invFourier}
        \end{equation}

\subsection{CONJUGATE GRADIENT METHOD}
\label{sub:conjugate_gradient_method}
    Suppose we have a linear system defined as
        \begin{equation}
            A\vec{x} = \vec{b}
            \label{eq:cglinsys}
        \end{equation}
    where $A$ is a $nxn$ real, symmetric and positive definite matrix while
    $\vec{x}$ and $\vec{b}$ are a non-zero real vectors. \\
    Define now the conjugate directions of two vectors
    $\vec{x}_i,\vec{x}_j:\mathbb{R}\rarr\mathbb{R}$ giving the constraint
        \begin{equation}
            \vec{x}_i^{T}A\vec{x}_j = \vec{0}
            \label{eq:cgconstr}
        \end{equation}
    This means that in the iterative process of finding $\vec{x}$ we perform
    searches within the conjugate directions of $\vec{x}$. \\
    Since the conjugate constraint(\Autoref{eq:cgconstr}) is defined by an
    inner product within the space $A$ is defined, the two vectors $\vec{x}_i$
    and $\vec{x}_j$ are orthogonal. If we now define a span $P$
        \begin{equation}
            P = \left\{\vec{p}_1,\dots,\vec{p}_n\right\}
            \label{eq:cgSpan}
        \end{equation}
    consisting of $n$ mutually orthogonal conjugate directions $\vec{p}_i$, we
    have a basis for $\mathbb{R}^n$. Expanding the solution $x_{i+1}$ to
    \Autoref{eq:cglinsys} in the mentioned basis
        \begin{equation}
            \vec{x} = \suml{i}{n} C_i\vec{p}_i
            \label{eq:cgnewstepbasisexpan}
        \end{equation}
    the linear system can be rewritten as
        \begin{equation}
            A\vec{x} = \suml{i}{n} C_iA\vec{p}_i = \vec{b}
            \label{eq:cgnewsystem}
        \end{equation}
    Giving the inner product
        \begin{equation}
            \vec{p}^T_k A\vec{x} = \suml{i}{n} C_i\vec{p}^T_kA\vec{p}_i
            \label{eq:cgnewinner}
        \end{equation}
    and we can define the coefficients $C_k$ as
        \begin{equation}
            C_k = \frac{\vec{p}^T_k\vec{b}}{\vec{p}^T_kA\vec{p}_k}
            \label{eq:cgNewCoeff}
        \end{equation}
    The problem at hand is then to choose a sequence of $n$ conjugate
    directions $P$ and compute the coefficients $C_k$.

\subsubsection{Iterative Method}
\label{ssub:iterative_method}
    The conjugate gradient method can in a similar manner be used on an
    iterative basis. We start with an initial guess $\vec{x}_0$ for the
    solutions and consider the linear system
        \begin{equation}
            A\vec{x} = \vec{b} - \vec{r}
            \label{eq:cgitlinsys}
        \end{equation}
    where $A,\vec{x}$ and $\vec{b}$ are defined as before and $\vec{r}$ is a
    so-called residual. \\
    Let $\vec{r}_k$ be the residual at the $k$-th step with a negative
    gradient. Using a similar approach as before we get that the conjugate
    direction $\vec{p}_{i+1}$ is
        \begin{equation}
            \vec{p}_{i+1} = \vec{r}_k -
            \frac{\vec{p}^T_kA\vec{r}_k}{\vec{p}^T_kA\vec{p}_k}\vec{p}_k
            \label{eq:cgitdirk}
        \end{equation}
    The iterative process is then to compute the directions and then solve the
    arising linear system by computing the coefficients as given in
    \Autoref{eq:cgNewCoeff}. \\
    The iterative process is then to compute
        \begin{equation}
            \vec{x}_{k+1} = \vec{x}_k + C_k\vec{p}_{k+1}
            \label{eq:cgitproc}
        \end{equation}
    with
        \begin{equation}
            C_k = \frac{\vec{p}^T_k\vec{r}_k}{\vec{p}^T_kA\vec{p}_k}
            \label{eq:cgitcoeff}
        \end{equation}
    The residual $\vec{r}_k$ at the $k$-th step is defined from
    \Autoref{eq:cgitlinsys} as
        \begin{equation}
            \vec{r}_k = \vec{b} - A\vec{x}_k
            \label{eq:cgitrk}
        \end{equation}

\subsection{LAPLACE EXPANSION}
\label{sub:laplace_expansion}
    Given a $n\times n$ matrix $M=[m_{ij}]$ and $i,j\in[1,\dots,n]$, then the
    determinant of $M$ is
        \begin{equation}
            \det(M) = \suml{j}{n}m_{ij}C_{ij}
            \label{eq:laplaceexp}
        \end{equation}
    where $C_{ij}$ is the $i,j$-element of the so-called cofactor matrix
    defined as the sub-matrix arising from removing the $i$-th row and $j$-th
    column from $M$. The cofactor matrix element is given as
        \begin{equation}
            C_{ij} = (-1)^{i+j}M_{ij}
            \label{eq:cofactor}
        \end{equation}
    where the indices $i$ and $j$ run from $1$ up to $n-1$ \cite{linalgDavid}.

\subsection{METROPOLIS-HASTINGS ALGORITHM}
\label{sub:metropolis_algorithm}
    The Metropolis algorithm bases itself on moves (also called transitions) as
    given in a Markov process(or Markov chain). Define a probability
    distribution function(PDF) $w_j(t)$ with a transition probability
    $\fij{w}{i}{j}$ which for a given time-step yields in the Markov formula
        \begin{equation}
            w_i(t+\veps) = \sum\limits_j \fij{w}{j}{i} w_j(t)
            \label{eq:MarkovFormula}
        \end{equation}
    The transition probability is defined with an acceptance probability
    distribution $\fij{A}{j}{i}$ and a proposed probability distribution
    $\fij{T}{j}{i}$ as
        \begin{equation}
            \fij{w}{j}{i} = \fij{A}{j}{i}\fij{T}{j}{i}
            \label{eq:wijdef}
        \end{equation}
    The acceptance $A$ is the probability for the move to be accepted and the
    proposal $T$ is different for each problem. \\
    In order for this transition chain to reach a desired convergence and
    reversibility we have the well known condition for detailed balance
    >>INSERT REF<<. This condition gives us that the probability distribution
    functions satisfy the following condition
        \begin{equation}
            w_i \ufij{T}{i}{j}\ufij{A}{i}{j} = w_j \ufij{T}{j}{i}\ufij{A}{j}{i}
            \Rarr \frac{w_i}{w_j} =
            \frac{\ufij{T}{j}{i}\ufij{A}{j}{i}}{\ufij{T}{i}{j}\ufij{A}{i}{j}}
            \label{eq:detailedBalance}
        \end{equation}
    We now need to choose an acceptance which fulfills
    \Autoref{eq:detailedBalance} and a common choice is the Metropolis
    condition
        \begin{equation}
            \ufij{A}{j}{i} = \min\left(1,
            \frac{w_i\ufij{T}{i}{j}}{w_j\ufij{T}{j}{i}}\right)
            \label{eq:MetropolisCondition}
        \end{equation}
    The Metropolis-Hastings algorithm is thus
        \begin{enumerate}[label=(\roman*)]
            \item Pick initial state $i$ at random.
            \item Pick proposed state at random in accordance to
                $\ufij{T}{j}{i}$.
            \item Accept state according to $\ufij{A}{j}{i}$.
            \item Jump to step (ii) until a specified number of states have
                been generated.
            \item Save the state $i$ and jump to step (ii).
        \end{enumerate}

\subsection{VARIATIONAL PRINCIPLE}
\label{sub:variational_principle}
    The variational principle states the following restriction on the ground
    state energy for a given symmetry
        \begin{equation}
            E_0 \leq \ecp{E[\Phi_T]} = \int \phi^{*}_T\hat{H}\phi_T\md\tau =
            \bra{\phi_T}\hat{H}\ket{\phi_T}
            \label{eq:HFvarE}
        \end{equation}
    that is the ground state energy $E_0$ is bounded by the expectation value
    of the trial energy.

\subsection{IMPORTANCE SAMPLING}
\label{sub:importance_sampling}
    In order to use the Metropolis algorithm as explained in
    \Autoref{sub:metropolis_algorithm}, we need to find the proposal
    probability distribution labeled $\ufij{T}{j}{i}$. This is what is known as
    importance sampling. \\
    This section will derive the importance sampling by using the Fokker-Planck
    equation for one particle
        \begin{equation}
            \prd{t}[P] = D\prd{x} \left(\prd{x} - F\right)P(x,t) 
            \label{eq:FokkerPlanc}
        \end{equation}
    where $F$ is a drift term and $D$ is a diffusion constant, and the Langevin
    equation
        \begin{equation}
            \prd{t}[x(t)] = DF(x(t)) + \eta
            \label{eq:Langevin}
        \end{equation}
    where $\eta$ is a Gaussian distributed random variable.

\subsubsection{Quantum Force}
\label{ssub:quantum_force}
    Since we are working with a isotropic diffusion characterized by a
    time-dependant probability density our system must obey the summed total
    Fokker-Planck equation
        \begin{equation}
            \prd{t}[P] = \sum\limits_iD\prd{x_i}\left(\prd{x_i} -
            F_i\right)P(x,t)
            \label{eq:FokkerPlancTotal}
        \end{equation}
    where $F_i$ is now the i'th component of the drift velocity term(given by
    an external potential). Since the probability is assumed to be convergent,
    that is it converges to a stationary probability density the time
    dependence at this point is zero for all $i$. We also know that the drift
    should be of form $F=g(x)\prd{x}[P]$ giving
        \begin{equation}
            \prd{x_i}[P][2] = P\prd{P}[g]\left(\prd{x_i}[P]\right)^2 +
            Pg\prd{x_i}[P][2] + g\left(\prd{x_i}[P]\right)^2
            \label{eq:FokkerPlancStationary}
        \end{equation}
    Now we may use the condition for stationary density meaning the left hand
    side of \Autoref{eq:FokkerPlancStationary} must equal zero giving that
    $g=1/P$(only possibility the derivatives cancel). Inserting in
    $P=\psi_T$(see >>INSERT REF<<) we get that the expression for the quantum
    force is
        \begin{equation}
            F = \frac{2}{\Psi_T}\nabla\Psi_T
            \label{eq:quantumForce}
        \end{equation}

\subsubsection{Solution}
\label{ssub:Solution}
    Using Eulers method(Euler-Maruyama method>>INSERT REF<<) on the Langevin
    equation(\Autoref{eq:Langevin}) one obtains the new positions
        \begin{equation}
            y = x + DF(x)\Delta t + \xi\sqrt{\Delta t}
            \label{eq:newPos}
        \end{equation}
    with $D=1/2$ in natural units due to the kinetic energy term and $\Delta t$
    is a time-step parameter. The random variable $\xi$ is within a Gaussian
    distribution of variance one and standard deviation zero. \\
    For the differential equation (\Autoref{eq:FokkerPlanc}) we insert in for
    the quantum force given in \Autoref{eq:quantumForce} and arrive at the
    following diffusion equation
        \begin{equation}
            \prd{t}[P] = -D\prd{r}[P][2]
            \label{eq:Pdiff}
        \end{equation}
    This equation is solved by using a Fourier transform in the spatial
    coordinate $r$ according to \Autoref{eq:FourierTransform} giving the equation
        \begin{equation}
            \prd{t}[\tilde{P}(s,t)] = -Ds^2\tilde{P}(s,t)
            \label{eq:PdiffFourier}
        \end{equation}
    with solution
        \begin{equation}
            \tilde{P}(s,\Delta t) = \tilde{P}(s,0) \me^{-Ds^2\Delta t}
            \label{eq:PdiffFourierSol}
        \end{equation}
    Initially the probability density is centered at $D\Delta tF(x)$, that is
    the drift term. This can be expressed terms of a Dirac-delta function,
    meaning 
        \begin{equation}
            P(y,x,0)=\delta(y-D\Delta tF(x))
            \label{eq:P0}
        \end{equation}
    where $y$ is given in \Autoref{eq:newPos}. Making an inverse transformation
    as described in \Autoref{eq:invFourier} and solving the subsequent
    transcendental integral yields in
        \begin{equation}
            P(y,x,\Delta t) = \frac{1}{\sqrt{4\pi D\Delta
            t}}\exp(-\frac{(y-x-D\Delta tF(x))^2}{4D\Delta t})
        \end{equation}
    which gives us the acceptance
        \begin{equation}
            \ufij{A}{y}{x} = \min\left(1, \frac{\abs{\psi_T(y)}^2P(y,x,\Delta
            t)} {\abs{\psi_T(x)}^2P(x,y,\Delta t)}\right)
            \label{eq:acceptanceImportance}
        \end{equation}
    We may rearrange the acceptance ratio giving
        \begin{equation}
            \frac{P(y,x,\Delta t)}{P(x,y,\Delta t)} = \exp(\frac{1}{2}
            \left(F(x)+F(y)\right) \left(\frac{D\Delta t}{2}\left(F(x) -
            F(y)\right) - y + x\right))
            \label{eq:accRearr}
        \end{equation}
    which is computationally less expensive.

\subsection{VMC}
\label{sub:vmc}
    This section will explain and derive the equations involved in the
    Variational Monte Carlo method. The whole section will assume that we have
    the following trial wave function, $\psi_T$
        \begin{equation}
            \psi_T(\vec{r}_1,\dots,\vec{r}_N) \equiv
            \det(\phi_1(\vec{r}_1,\alpha),\dots,\phi(\vec{r}_N,\alpha))
            \prod^N_{i<j} \exp(\frac{ar_{ij}}{1+\beta r_{ij}})
            \label{eq:psiT}
        \end{equation}
    with the $\vec{r}$'s being the position of the electrons and the $\phi$'s
    being the wave function to some known system(i.e harmonic oscillator). The
    position $r_{ij}$ is a relative distance $\abs{\vec{r}_i-\vec{r}_j}$ while
    $\alpha$ and $\beta$ are variational parameters and $a$ is a specific
    constant dependant of the total spin symmetry of electron $i$ and $j$ as
        \begin{equation}
            a = \left\{
                    \begin{aligned}
                        1, & \indent \text{anti-parallel spin} \\
                        \frac{1}{3}, & \indent \text{parallel spin}
                    \end{aligned}
                \right.
            \label{eq:a}
        \end{equation}
    This is also known as a Pade-Jastrow factor. \\
    We also define the total Hamiltonian of the system for the quantum dot case
    as
        \begin{equation}
            \hat{H} = \hat{H}_O + \hat{H}_I
            \label{eq:totalHamil}
        \end{equation}
    with $\hat{H}_O$ being the harmonic oscillator defined in
    \Autoref{eq:cartHarmOsc} and $\hat{H}_I$ being the Hamiltonian for the
    electron interactions(Coulomb interaction) defined as
        \begin{equation}
            \hat{H}_I = \sum_{i<j}\frac{1}{r_{ij}}
            \label{eq:hamilCou}
        \end{equation}
    Lastly, we work in natural units setting $\hbar=c=1$, and all the above
    equations(\Autoref{eq:psiT,eq:a,eq:totalHamil,eq:hamilCou}) also assume
    natural units.

\subsubsection{Expectation Value and Local Energy}
\label{ssub:EXPECTATION VALUE AND LOCAL ENERGY}
    Given the Hamiltonian \Autoref{eq:totalHamil} and a trial wave function
    $\Psi_T(R,\Lambda)$ and using the variational principle, as given in
    \Autoref{eq:HFvarE} the upper bound for the ground state energy $E_0$ if
    $H(r)$ is
        \begin{equation}
            E[\hat{H}(R,\Lambda)] \leq \ecp{\hat{H}} =
            \frac{\bra{\Psi_T}\hat{H}\ket{\Psi_T}}{\braket{\Psi_T}}
            \label{eq:HupperBound}
        \end{equation}
    where $R=(r_1,\dots,r_N)$ is the positions to $N$ particles and
    $\Lambda=(\lambda,\dots,\lambda_M)$ are the $M$ variational parameters. \\
    Now we can expand the trial wave function $\Psi_T(R,\Lambda)$ in the
    orthonormal eigenstates of the Hamiltonian $\hat{H}$(which form a complete
    set)
        \begin{equation}
            \Psi_T(r) = \sum_ic_i\Psi_i(r)
            \label{eq:PsiTExpand}
        \end{equation}
    and the upper bound given in \Autoref{eq:HupperBound} is
        \begin{equation}
            E_0 \leq \frac{\sum\limits_{ij}c_ic^{*}_j
            \bra{\Psi_j}\hat{H}\ket{\Psi_i}} {\sum\limits_{ij}c_ic^{*}_j
            \bra{\Psi_j}\ket{\Psi_i}} = \frac{\sum\limits_n
            a^2_nE_n}{\sum\limits_n a^2_n}
            \label{eq:HupperBoundExpand}
        \end{equation}
    where the eigenequation for the Hamiltonian $\hat{H}\Psi_n=E_n\Psi_n$ was
    used. The expression given in \Autoref{eq:HupperBound} is the expectation
    value we evaluate in each variational step that is we choose $\alpha$
    according to some minimization algorithm and re-evaluate the expectation
    value. \\
    In order to introduce the transition probability as given in the Metropolis
    algorithm(see \Autoref{sub:metropolis_algorithm}) the expectation value,
    \Autoref{eq:HupperBoundExpand}, needs to be rewritten in terms of a PDF. We
    can define this as
        \begin{equation}
            P(R) \equiv \frac{\abs{\Psi_T(R)}^2}{\int\abs{\Psi_T(R)}^2\md R}
            \label{eq:PDFdef}
        \end{equation}
    Now we observe that if we define a quantity
        \begin{equation}
            E_L(R,\Lambda) \equiv
            \frac{1}{\Psi_T(R,\Lambda)}\hat{H}\Psi_T(R,\Lambda)
            \label{eq:ELdef}
        \end{equation}
    which is the so-called local energy. The expectation value given in
    \Autoref{eq:HupperBoundExpand} can be rewritten as
        \begin{equation}
            E[H] = \int P(R)E_L(R,\Lambda) \md R \approx
            \frac{1}{N}\sum\limits_{i=1}^N P(r_i,\Lambda)E_L(r_i,\Lambda)
            \label{eq:ecpRew}
        \end{equation}
    which is of the form given in \Autoref{eq:MarkovFormula} and $N$ is the
    number of states(or Monte Carlo cycles).

\subsubsection{Analytical Expression for Local Energy}
\label{ssub:ANALYTICAL EXPRESSION FOR LOCAL ENERGY}
    We use the Metropolis algorithm to find an estimate for the expectation
    value to the energy. In this expression we have a so-called local energy
    defined as
        \begin{equation}
            E_L = \frac{1}{\psi_T}\hat{H}\psi_T
            \label{eq:ELana}
        \end{equation}
    This expression shows up in the integrand as the multiplied function to the
    PDF which is used in the Metropolis algorithm.

\subsubsection{Two Electron Case}
\label{ssub:Two Electron Case}
    We start by finding the local energy in the case with two electrons. The
    trial wave function is in this case(related to
    \Autoref{eq:cartUarmOscWavef}) using \Autoref{eq:psiT}
        \begin{equation}
            \psi_T(\vec{r}_1,\vec{r_2}) =
            A\exp(-\frac{\alpha\omega}{2}\left(r^2_1+r^2_2\right))
            \exp(\frac{ar_{12}}{1+\beta r_{12}})
            \label{eq:TECpsiT}
        \end{equation}
    Using the definition of the trial wave function, \Autoref{eq:ELdef} and the
    total Hamiltonian(\Autoref{eq:totalHamil}) the local energy with
    \Autoref{eq:ELdef} is
        \begin{equation}
            E_L = \frac{1}{\psi_T}\left(\hat{H}_O\psi_T +
            \hat{H}_I\psi_T\right)
            \label{eq:EL2}
        \end{equation}
    we solve the first part $\hat{H}_O\psi_T$
        \begin{equation}
            \hat{H}_O\psi_T = \frac{1}{2}\left(-\nabla^2_1 - \nabla^2_2 +
            \omega^2\left(r^2_1 + r^2_2\right)\right)\psi_T
            \label{eq:locF}
        \end{equation}
    Starting with the Laplacian for electron $1$ and solving the second
    derivative with respect to $x_1$ we have
        \begin{equation}
            \prd{x_1}[\psi_T][2] =
            A\exp(-\frac{\alpha\omega}{2}\left(r^2_2+y^2_1\right))
            \prd{x_1}[][2]\left[\exp(-\frac{\alpha\omega}{2}x^2_1 +
            \frac{a}{\beta + \frac{1}{r_{12}}})\right]
            \label{eq:derx1first}
        \end{equation}
    Starting with the first derivative in the exponential we get
        \begin{equation}
            \left.
                \begin{aligned}
                    \prd{x_1} \left[-\frac{\alpha\omega}{2}x^2_1\right] &=
                    -\alpha\omega x_1 \\
                    \prd{x_1} \left[\frac{a}{\beta + \frac{1}{r_{12}}}\right]
                    &= \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})^2}
                \end{aligned}
            \right\}\Rightarrow
            \prd{x_1}[\psi_T] = \left(-\alpha\omega x_1 +
            \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})^2}\right)\psi_T
            \label{eq:derx1first2}
        \end{equation}
    meaning \Autoref{eq:derx1first} is
        \begin{align}
            \prd{x_1}[\psi_T][2] &=
            A\exp(-\frac{\alpha\omega}{2}\left(r^2_2+y^2_1\right))
            \prd{x_1}\left[-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1 +
            \beta r_{12})^2}\right] \exp(-\frac{\alpha\omega}{2}x^2_1 +
            \frac{a}{\beta + \frac{1}{r_{12}}}) \nonumber \\
            &= \prd{x_1}\left[\left(-\alpha\omega x_1 +
            \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})}\right)\psi_T\right]
            \label{eq:derx1first3}
        \end{align}
    Using the product rule for differentiation and starting with the first
    expression we get that
        \begin{equation}
            \prd{x_1}\left[-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1 +
            \beta r_{12})^2}\right] = -\alpha\omega + \frac{a}{r_{12}(1 +
            \beta r_{12})^2} - \frac{a(x_1-x_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1+\beta r_{12})^3}
            \label{eq:derx1first4}
        \end{equation}
    giving
        \begin{align}
            \prd{x_1}[\psi_T][2] &= \psi_T\prd{x_1}\left[-\alpha\omega x_1 +
            \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})^2}\right] +
            \left(-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1 + \beta
            r_{12})^2}\right)\prd{x_1}[\psi_T] \nonumber \\ 
            &= \left[-\alpha\omega + \frac{a}{(1+\beta r_{12})^2} -
            \frac{a(x_1-x_2)^2(1+3\beta r_{12})}{r^3_{12}(1 + \beta r_{12})^3}
            + \left(-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1+\beta
            r_{12})^2}\right)^2\right]\psi_T
            \label{eq:derx1first5}
        \end{align}
    The second derivative with respect to $y_1$ yields with a similar
    derivation
        \begin{equation}
            \prd{y_1}[\psi_T][2] = \left[-\alpha\omega + \frac{a}{(1+\beta r_{12})^2} -
            \frac{a(y_1-y_2)^2(1+3\beta r_{12})}{r^3_{12}(1 + \beta r_{12})^3}
            + \left(-\alpha\omega y_1 + \frac{a(y_1-y_2)}{r_{12}(1+\beta
            r_{12})^2}\right)^2\right]\psi_T
            \label{eq:dery1}
        \end{equation}
    The second derivatives with respect to $x_2$ and $y_2$, are derived in a
    similar manner, only we get a change in signs when differentiating
    $r_{12}$. This gives
        \begin{equation}
            \prd{x_2}[\psi_T][2] = \left[-\alpha\omega +
            \frac{a}{r_{12}(1+\beta r_{12})^2} - \frac{a(x_1-x_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1+\beta r_{12})^3} + \left(\alpha\omega x_2 +
            \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2}\right)^2\right]\psi_T
            \label{eq:derx2}
        \end{equation}
    and
        \begin{equation}
            \prd{y_2}[\psi_T][2] = \left[-\alpha\omega +
            \frac{a}{r_{12}(1+\beta r_{12})^2} - \frac{a(y_1-y_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1+\beta r_{12})^3} + \left(\alpha\omega y_2 +
            \frac{a(y_1-y_2)}{r_{12}(1+\beta r_{12})^2}\right)^2\right]\psi_T
            \label{eq:dery2}
        \end{equation}
    gathering \Autoref{eq:derx1first5,eq:dery1,eq:derx2,eq:dery2} we get
        \begin{align}
            (\nabla^2_1 + \nabla^2_2)\psi_T &= \prd{x_1}[\psi_T][2] +
            \prd{y_1}[\psi_T][2] + \prd{x_2}[\psi_T][2] + \prd{y_2}[\psi_T][2]
            \nonumber \\
            &= \left[-4\alpha\omega + \frac{4a}{r_{12}(1+\beta r_{12})^2} -
            \frac{2a(1+3\beta r_{12})}{r_{12}(1+\beta r_{12})^3} +
            \alpha^2\omega^2\left(r^2_1 + r^2_2\right) - \frac{2a\alpha\omega
            r_{12}}{(1+\beta r_{12})^2} + \frac{2a^2}{(1+\beta
            r_{12})^4}\right]\psi_T \\
            &= \left[\alpha^2\omega^2\left(r^2_1 + r^2_2\right) - 4\alpha\omega
            - \frac{2a\alpha\omega r_{12}}{(1+\beta r_{12})^2} +
            \frac{2a}{(1+\beta r_{12})^2}\left(\frac{a}{(1+\beta r_{12})^2} +
            \frac{1}{r_{12}} - \frac{2\beta}{1+\beta
            r_{12}}\right)\right]\psi_T
            \label{eq:derLast}
        \end{align}
    and the local energy(\Autoref{eq:EL2}) is finally
        \begin{equation}
            E_L = \frac{1}{2}\omega^2\left(1 - \alpha^2\right)\left(r^2_1 +
            r^2_2\right) + 2\alpha\omega - \frac{a}{(1+\beta
            r_{12})^2}\left(\frac{a}{(1+\beta r_{12})^2} - \alpha\omega r_{12}
            + \frac{1}{r_{12}} - \frac{2\beta}{1+\beta r_{12}}\right) +
            \frac{1}{r_{12}}
            \label{eq:EL2Final}
        \end{equation}

\subsubsection{General Case}
\label{ssub:general_case}
\newcommand{\gJ}{\exp(\frac{a}{\beta + \frac{1}{r_{ij}}})}
\newcommand{\gk}{\exp(\frac{a}{\beta + \frac{1}{r_{kj}}})}
\newcommand{\prodsecrule}[2]{2\left(\prd{x_k}[#2]\prd{x_k}[#1] +
\prd{y_k}[#2]\prd{y_k}[#1]\right) + {#2} \nabla^2_k {#1} + {#1} \nabla^2_k
{#2}}
    For the general case with $N$ electrons(still closed shell) the local
    energy is defined as in \Autoref{eq:EL2}, but with $\psi_T$ defined as in
    \Autoref{eq:psiT}. The Laplacian in this case would then be
        \begin{equation}
            \nabla^2_N = \suml{k}{N} \nabla^2_k = \suml{k}{N}
            \left(\prd{x_k}[][2] + \prd{y_k}[][2]\right)
            \label{eq:nablaN}
        \end{equation}
    that is a sum over the single-particle spacial Laplacians. \\
    We define a new function $g(\vec{r},\beta)$ as the Jastrow-factor
        \begin{equation}
            g(\vec{r},\beta) \equiv \prod_{i<j} \gJ
            \label{eq:g}
        \end{equation}
    and apply \Autoref{eq:nablaN} to the trial wavefunction $\psi_T$ giving
        \begin{align}
            \nabla^2_N\psi^2_T &=
            \suml{k}{N}\nabla^2_k\left[\det(\Phi(\vec{r},\alpha))
            g(\vec{r},\beta)\right] \nonumber \\
            &= \suml{k}{N} \left(\prodsecrule{g(\vec{r},\beta)}
            {\det(\Phi(\vec{r},\alpha))}\right)
            \label{eq:lapPsiT1}
        \end{align}
    We have used a short-hand $\det(\Phi)$ defined as
        \begin{equation}
            \det(\Phi(\vec{r},\alpha)) \equiv
            \det(\phi_1(r_1),\dots,\phi_N(r_N))
            \label{eq:Phidef}
        \end{equation}
    and used the product rule for differentiation. \\
    Starting by solving $\nabla^2_k\det(\Phi(\vec{r},\alpha))$ and write out
    the determinant with the Laplace expansion given in
    \Autoref{sub:laplace_expansion}.
        \begin{align}
            \nabla^2_k\det(\Phi(\vec{r},\alpha)) &=
            \nabla^2_k\suml{i}{N}\Phi_{ij}C_{ij} \nonumber \\
            &= \suml{k}{N}\nabla^2_k\left(\Phi_{kj}C_{kj}\right) \nonumber \\
            &= \suml{k}{N}\left(\prodsecrule{\Phi_{kj}}{C_{kj}}\right)
            \nonumber \\
            &= \suml{k}{N}C_{kj} \nabla^2_k \Phi_{kj}
            \label{eq:PhiLapexp}
        \end{align}
    The latter equality is due to the fact that the cofactor to row $k$ is not
    dependant on the position of particle $k$ meaning the differential is zero
    for all $k$. \\
    The single particle wavefunctions are given by the Harmonic oscillator wave
    functions, but with the added variational parameter $\alpha$ in the
    exponential meaning we have
        \begin{equation}
            \Phi_{kj} = \phi_{n_{x_k}n_{y_k}}(r_k,\alpha) =
            H_{n_{x_k}}\left(\sqrt{\omega}x_k\right)
            H_{n_{y_k}}\left(\sqrt{\omega}y_k\right)
            \exp(-\frac{\alpha\omega}{2}\left(x^2_k + y^2_k\right))
            \label{eq:Phikj}
        \end{equation}
    The expression to be solved for the $k$-th particle is thus
        \begin{equation}
            \nabla^2_k \Phi_{kj} = \exp(-\frac{\alpha\omega}{2}y^2_k)
            \prd{x_k}[][2]\left(H_{n_{x_k}}\left(\sqrt{\omega}x_k\right)
            \exp(-\frac{\alpha\omega}{2}x^2_k)\right) +
            \exp(-\frac{\alpha\omega}{2}x^2_k)
            \prd{y_k}[][2]\left(H_{n_{y_k}}\left(\sqrt{\omega}y_k\right)
            \exp(-\frac{\alpha\omega}{2}y^2_k)\right)
            \label{eq:nablakth}
        \end{equation}
    Solving the differential for $x_k$ and similarly $y_k$ by substituting
    $s=\sqrt{\omega}x_k=\sqrt{\omega}y_k$ and labeling $n_x,n_y\rarr n$ and
    defining
        \begin{equation}
            \begin{aligned}
                e(x_k) &\equiv \exp(-\frac{\alpha\omega}{2}x^2_k) \\
                e(y_k) &\equiv \exp(-\frac{\alpha\omega}{2}y^2_k)
            \end{aligned}
            \label{eq:e}
        \end{equation}
    Using the product rule for differentiation and \Autoref{eq:hermiteDer} for
    the derivative of the Hermite polynomials with the recursion relation given
    in \Autoref{eq:hermiteReq} for $H_n(s)$ gives
        \begin{align}
            \omega \prd{s}[][2]\left(H_n(s)e(s))\right) &=
            \omega\left(\prd{x}[H_n][2] - 2\alpha s\prd{x}[H] +
            \alpha\left(\alpha s^2 - 1\right)H_n(s)\right)e(s) \nonumber \\
            &= \omega\left(8n(n-1)H_{n-2}(s) - 4n\alpha sH_{n-1}(s) +
            \alpha(\alpha s^2-1)(2sH_{n-1}(s)-2(n-1)H_{n-2})\right)e(s)
            \nonumber \\
            &= \omega \left[2(n-1)\left(4n - \alpha\left(\alpha s^2 -
            1\right)\right)H_{n-2}(s) + 2\alpha s\left(\alpha s^2 - 1 -
            2n\right)H_{n-1}(s)\right]e(s) \nonumber \\
            &= \omega \left[2(n-1)\left(4n - \alpha\left(\alpha s^2 -
            1\right)\right)H_{n-2}(s) + \alpha\left(\alpha s^2 - 1 -
            2n\right)\left(H_n(s) + 2(n-1)H_{n-2}(s)\right)\right]e(s)
            \nonumber \\
            &= \omega \left[2n(n-1)(2-\alpha)\frac{H_{n-2}}{H_n} +
            \alpha\left(\alpha s^2 - 1 - 2n\right)\right]H_n(s)e(s)
            \label{eq:diffs2}
        \end{align}
    We have in the latter step used \Autoref{eq:hermiteReq} to find $H_{n-1}$.
    \\
    Defining
        \begin{equation}
            \Theta_{n_{x_k}n_{y_k}}(x_k,y_k,\alpha) \equiv 2\omega
            \left(2-\alpha\right)\left[n_{x_k}\left(n_{x_k}-1\right)
            \frac{H_{n_{x_k}-2}}{H_{n_{x_k}}} + n_{y_k}\left(n_{y_k}-1\right)
            \frac{H_{n_{y_k}-2}}{H_{n_{y_k}}}\right] + \alpha\omega\left(\alpha
            \omega r_k^2 - 2\left(n_{x_k} + n_{y_k} + 1\right)\right)
            \label{eq:Ddef}
        \end{equation}
    gives
        \begin{equation}
            \nabla^2_k\Phi_{kj} =
            \Theta_{n_{x_k}n_{y_k}}(x_k,y_k,\alpha)\Phi_{kj}(r_k,\alpha)
            \label{eq:nablakthfinal}
        \end{equation}
    which gives that \Autoref{eq:PhiLapexp} is
        \begin{equation}
            \suml{k}{N}C_{kj}\nabla^2_k\Phi_{kj} = \det(\Phi(\vec{r},\alpha))
            \suml{k}{N}\Theta_{n_{x_k}n_{y_k}}(x_k,y_k,\alpha)
            \label{eq:PhiLapExpfin}
        \end{equation}
    The Laplacian for $g$ is derived as follows
        \begin{align}
            \nabla^2_k g(\vec{r},\beta) &= \nabla^2_k \prod_{i<j} \gJ \nonumber
            \\
            &= \left(\prod_{i<j,i\neq k} \gJ\right)
            \nabla^2_k\left(\prod_{j\neq k} \exp(\frac{a}{\beta +
            \frac{1}{r_{kj}}})\right) \nonumber \\
            &= \left(\prod_{i<j,i\neq k} \gJ\right) \left(\prod_{j\neq k}
            \exp(\frac{a}{\beta + \frac{1}{r_{kj}}})\right)
            \suml{j}{N}\nabla^2_k\left(\frac{a}{\beta +
            \frac{1}{r_{kj}}}\right) \\
            \label{eq:lapg}
        \end{align}
    and solve for the $k$-th part of the sum
        \begin{align}
            \nabla^2_k\left(\frac{a}{\beta + \frac{1}{r_{kj}}}\right) &=
            \prd{x_k}[][2]\left(\frac{a}{\beta + \frac{1}{r_{kj}}}\right) +
            \prd{y_k}[][2]\left(\frac{a}{\beta + \frac{1}{r_{kj}}}\right)
            \nonumber \\
            &= \frac{2a}{r_{kj}(1+\beta r_{kj})^2} - \frac{a(1+3\beta
            r_{kj})}{r_{kj}(1+\beta r_{kj})^3} + \frac{a^2}{(1+\beta r_{kj})^4}
            \nonumber \\
            &= \frac{a}{(1+\beta r_{kj})^2}\left(\frac{1}{r_{kj}} -
            \frac{2\beta}{1+\beta r_{kj}} + \frac{a}{(1+\beta r_{kj})^2}\right)
            \label{eq:derxkg}
        \end{align}
    We have just reused the results from \Autoref{eq:derx1first2,eq:dery1} in
    the differentiation above. \\
    We define a new function
        \begin{equation}
            Q\left(r_k,\beta\right) \equiv \suml{j}{N} \frac{a}{(1+\beta
            r_{kj})^2}\left(\frac{1}{r_{kj}} - \frac{2\beta}{1+\beta r_{kj}} +
            \frac{a}{(1+\beta r_{kj})^2}\right)
            \label{eq:Qdef}
        \end{equation}
    and insert this into \Autoref{eq:lapg} yielding
        \begin{equation}
            \nabla^2_k g\left(\vec{r},\beta\right) =
            g\left(\vec{r},\beta\right)Q\left(r_k,\beta\right)
            \label{eq:lapgfin}
        \end{equation}
    For the cross-term in \Autoref{eq:lapPsiT1} we need the single derivatives.
    Starting with $\det(\Phi(\vec{r},\alpha))$ and using a similar approach as
    in \Autoref{eq:PhiLapexp} with \Autoref{eq:diffs2} gives
        \begin{align}
            \prd{s}[\det(\Phi)] &=
            \suml{k}{N}C_{kj}\left(\prd{s}[\Phi_{kj}]\right) \nonumber \\
            &= \suml{k}{N}C_{kj}\Phi_{kj}
            \sqrt{\omega}\left(2n\frac{H_{n-1}}{H_n} - \alpha s\right)
            \nonumber \\
            &= \det(\Phi(\vec{r},\alpha))\suml{k}{N}
            \sqrt{\omega}\left(2n\frac{H_{n-1}}{H_n} - \alpha s\right)
            \label{eq:detder1}
        \end{align}
    For $g(\vec{r},\beta)$ we have with \Autoref{eq:lapg,eq:derxkg}
        \begin{align}
            \prd{x_k}{g(\vec{r},\alpha)} &= \prd{x_k}\left(\prod_{i<j}
            \gJ\right) \nonumber \\
            &= g(\vec{r},\beta) \suml{j}{N}\prd{x_k}\left(\frac{a}{\beta +
            \frac{1}{r_{kj}}}\right) \nonumber \\
            &= g(\vec{r},\beta) \suml{j}{N} \frac{a\left(x_k-x_j\right)}
            {r_{kj}\left(1+\beta r_{kj}\right)^2}
            \label{eq:gder1}
        \end{align}
    where $\Upsilon$ is defined to be
        \begin{equation}
            \Upsilon_{n_{x_k}n_{y_k}}(x_k,y_k,\alpha) \equiv
            \sqrt{\omega}\left(2\left(n_{x_k}\frac{H_{n_{x_k}-1}}{H_{n_{x_k}}}
            + n_{y_k}\frac{H_{n_{y_k}-1}}{H_{n_{y_k}}}\right) -
            \alpha\sqrt{\omega}\left(x_k+y_k\right)\right)
            \label{eq:upsilondef}
        \end{equation}
    Which gives the final expression for the Laplacian for our wavefunction
        \begin{equation}
            \nabla^2_N\Psi_T = \suml{k}{N}
            \left(\Theta_{n_{x_k}n_{y_k}}(x_k,y_k,\alpha) +
            Q\left(r_k,\beta\right)\right)\Psi_T
            \label{eq:lapNfin}
        \end{equation}
    and the local energy is thus
        \begin{equation}
            E_L = \frac{1}{2}\suml{k}{N} \left(\omega^2r_k -
            \Theta_{n_{x_k}n_{y_k}}(x_k,y_k,\alpha) +
            Q\left(r_k,\beta\right)\right) + \sum^N_{i<j} \frac{1}{r_{ij}}
            \label{eq:ELNfin}
        \end{equation}

\subsection{Optimization}
\label{sub:optimization}
    In the method described in \Autoref{sub:vmc} the heavy load in terms of
    calculation lies within the calculation of the determinant ratio given in
    \Autoref{eq:acceptanceImportance,eq:quantumForce}. This section will derive
    an expression for these ratios in terms of computation time. We will in the
    whole section assume we only move one particle at a time in the Monte Carlo
    cycle.

\subsubsection{Determinant Ratio}
\label{ssub:determinant_ratio}
\newcommand{\rnew}{r^{\text{new}}}
\newcommand{\rold}{r^{\text{old}}}
    In the Metropolis algorithm we calculate a ratio of determinants in the
    Metropolis test. Starting by defining a Slater determinant Matrix $D$ with
    entries defined as
        \begin{equation}
            D_{ij} \equiv \phi_j({r_i})
            \label{eq:Ddef}
        \end{equation}
    where the $\phi$'s are defined as in \Autoref{sub:vmc}. \\
    In terms of cofactors $C_{ij}$ we have
        \begin{equation}
            \det(D) = \sum\limits^N_{j=1} D_{ij}C_{ji}
            \label{eq:Dcofac}
        \end{equation}
    If we now take into light the mentioned assumption about moving only one
    particle at a time the determinant given in \Autoref{eq:Ddef} only gets a
    change of one row. \\
    Defining the ratio as
        \begin{equation}
            R \equiv \frac{\det(D\left(x^{\text{new}}\right))}
            {\det(D\left(x^{\text{old}}\right))}
            \label{eq:Rdef}
        \end{equation}
    Using the fact that when moving the particle at position $r_i$ the
    cofactors remain unchanged and inserting in \Autoref{eq:Dcofac} into
    \Autoref{eq:Rdef} we have
        \begin{equation}
            R = \frac{\sum\limits^{N}_{j=1} D_{ij}(\rnew)C_{ji}(\rold)}
            {\sum\limits^{N}_{j=1} D_{ij}(\rold)C_{ij}(\rold)}
            \label{eq:R1}
        \end{equation}
    Since the Slater is square(closed shell), we have the following ratio
    >>INSERT REF<<
        \begin{equation}
            \det(D) = \frac{D^{\dagger}}{D}
            \label{eq:Ddagger}
        \end{equation}
    inserting this into \Autoref{eq:R1} we get
        \begin{equation}
            R = \frac{\suml{j}{N}D_{ij}(\rnew)D^{-1}_{ji}(\rold)\abs{D}}
            {\suml{j}{N}D_{ij}(\rold)D^{-1}_{ji}(\rold)\abs{D}}
            \label{eq:Rrat}
        \end{equation}
    Since $D$ is ivertible >>INSERT REF<< we have
        \begin{equation}
            \suml{k}{N}D_{ik}D^{-1}_{kj} = \delta_{ij}
            \label{eq:Dorth}
        \end{equation}
    meaning the denominator in \Autoref{eq:Rrat} is equal to $1$ and the ratio
    is finally(with \Autoref{eq:Ddef} inserted)
        \begin{equation}
            R = \suml{j}{N}\phi_j(\rnew_i)\phi^{-1}_{ji}(\rold)
            \label{eq:Rfinal}
        \end{equation}
    We can follow a similar approach to find the ratio given in
    \Autoref{eq:quantumForce} for the quantum force. The expression is simply
        \begin{equation}
            \frac{\nabla_i\abs{D(r_i)}}{\abs{D(r_i)}} =
            \suml{j}{N}\nabla_i\phi_j(\rnew_i)\phi^{-1}_{ji}(\rold)
            \label{eq:Rder}
        \end{equation}

\subsection{Inverse of Matrix}
\label{sub:inverse_of_matrix}
    In \Autoref{ssub:determinant_ratio} we derived a formula for calculating
    the ratio of determinants by the inverse of the old determinant when only
    one row is changed. This section gives a formula for updating the inverse
    of a matrix in that case. The formula is
        \begin{equation}
            D^{-1}_{kj}\left(\rnew\right) = 
                \left\{\begin{aligned}
                    &D^{-1}_{kj}\left(\rold\right) -
                    \frac{D^{-1}_{ik}\left(\rold\right)}{R}
                    \suml{l}{N}D_{il}\left(\rnew\right)D^{-1}_{lj}\left(\rold\right),&
                    j \neq i \\
                    &\frac{D^{-1}_{ik}\left(\rold\right)}{R}
                    \suml{l}{N}D_{il}\left(\rold\right)D^{-1}_{lj}\left(\rold\right),&
                    j = i
                \end{aligned}\right.
            \label{eq:InvUpdate}
        \end{equation}
    as described by Sherman and Morris \cite{vmc,shermorInv}. 

\subsection{STATISTICAL ANALYSIS}
\label{sub:statistical_analysis}
    Since Monte Carlo simulations can be considered to be computer experiments
    the resulting data can be analysed with the same statistical theory as one
    would with experimental data. This section will give a brief overview of
    some of the statistical concepts and explain the method of Blocking(for
    estimating the standard deviation).

\subsubsection{STANDARD DEVIATION, VARIANCE AND COVARIANCE}
\label{ssub:standard_deviation__variance_and_covariance}
    Given a PDF $P(x)$ the mean value, variance and covariance is
        \begin{align}
            \bar{x}_n &\equiv \frac{1}{n}\suml{k}{n} x_k \\
            \text{var}(x) &\equiv
            \frac{1}{n}\suml{k}{n}\left(x_k-\bar{x}_n\right)^2 \\
            \text{cov}(x) &\equiv \frac{1}{n}\sum\limits_{kl}
            \left(x_k-\bar{x}_n\right)\left(x_l-\bar{x}_n\right)
            \label{eq:statfinite}
        \end{align}
    for a finite size sample.

\subsubsection{Central Limit Theorem}
\label{ssub:central_limit_theorem}
    The central limit theorem states that given a PDF $P_{X_n}$ for a sample $X_n$
    the mean value can be expressed as
        \begin{equation}
            \lim\limits_{n\rarr\infty}p_{\bar{X}_n}(x) =
            \sqrt{\frac{n}{2\pi\sigma(X)^2}}
            \exp(-\frac{n(x-\bar{X}_n)^2}{2\sigma(X)^2})
            \label{eq:centrallimit}
        \end{equation}
    where $\sigma^2(X)$ is the variance of the sample.

\subsubsection{Statistical Error}
\label{ssub:statistical_error}
    The error in a sample is just the spread of the mean, i.e the variance of
    said sample. Set a finite size sample $X_n$ the error would be
        \begin{equation}
            \sigma^2_X = \text{var}\left(\bar{X}_n\right) =
            \frac{1}{n^2}\sum\limits_{ij}\text{cov}\left(X_i,X_j\right)
            \label{eq:error}
        \end{equation}
    Using the central limit theorem we can approximate the real mean as
        \begin{equation}
            \ecp{x_i} \approx \frac{1}{n}\suml{k}{n} x_k  = \bar{x}
            \label{eq:meanapprox}
        \end{equation}
    which gives the approximative covariance
        \begin{equation}
            \text{cov}\left(X_i,X_j\right) \approx
            \ecp{(x_i-\bar{x})(x_j-\bar{x})} = \frac{1}{n}\text{cov}(x)
            \label{eq:covapprox}
        \end{equation}
    and the error is thus
        \begin{equation}
            \sigma^2_X = \frac{1}{n}\text{cov}(x)
            \label{eq:errorfinal}
        \end{equation}
    We can split this equation in part giving
        \begin{align}
            \sigma^2_X &= \frac{1}{n}\text{var}(x) +
            \frac{1}{n}\left(\text{cov}(x) - \text{var}(x)\right) \nonumber \\
            &= \frac{1}{n^2}\suml{k}{n} \left(x_k - \bar{x}_n\right) +
            \frac{2}{n^2}\sum\limits_{k<l}
            \left(x_k-\bar{x}_n\right)\left(x_l-\bar{x}_n\right)
            \label{eq:errorSplit}
        \end{align}

\subsubsection{Autocorrelation Function}
\label{ssub:auto-correlation_function}
    Writing the second term in our error given in \Autoref{eq:errorSplit} (the
    so-called correlation term) as a partial sums gives
        \begin{equation}
            f_d = \frac{1}{n-d}\suml{k}{n-d}
            \left(x_k-\bar{x}_n\right)\left(x_{k+d}-\bar{x}_n\right)
            \label{eq:autocorrf}
        \end{equation}
    which this we can define the autocorrelation function $k_d$ as
        \begin{equation}
            k_d \equiv \frac{f_d}{\text{var}(x)}
            \label{eq:autocorrfinal}
        \end{equation}
    The sample error is now
        \begin{equation}
            \sigma^2 = \frac{\tau}{n}\text{var}(x)
            \label{eq:errorauto}
        \end{equation}
    with $\tau$ defined as the autocorrelation time
        \begin{equation}
            \tau \equiv 1 + 2\suml{d}{n-1}k_d
            \label{eq:autocorrtime}
        \end{equation}

    The method of blocking estimates the standard deviation given in

\section{SETUP}
\label{sec:setup}
\section{RESULTS}
\label{sec:results}
\section{DISCUSSION}
\label{sec:discussion}
\section{CONCLUSION}
\label{sec:conclusion}

\begin{thebibliography}{}
    \bibitem{GriffQuan}
        David J. Griffiths Chapters 2, 4 and 7,
        \textit{Introduction to Quantum Mechanics Second Edition},
        Pearson, 2005, ISBN 0-13-111892-7.
    \bibitem{basicMB}
        Morten Hjort-Jensen,
        \textit{Computational Physics: Hartree-Fock methods and introduction to Many-Body Theory.} \\
        \url{https://www.github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/pub/basicMB/pdf}, 2017
    \bibitem{vmc}
        Morten Hjort-Jensen,
        \textit{Computational Physics: Variational Monte Carlo methods.} \\
        \url{https://www.github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/pub/vmc/pdf}, 2017
    \bibitem{taut}
        M. Taut, 
        \textit{Two Electrons in an External Oscillator Potential; Particular analytic solutions of a Coulomb Correlation Problem},
        Physical Review A 48, 3561, 1993.
    \bibitem{NeOr}
        Negele and Orland,
        \textit{Quantum Many-Particle Systems},
        Addison-Wesley.
    \bibitem{FeWa}
        Fetter and Walecka,
        \textit{Quantum Theory of Many-Particle Systems},
        Mcgraw-Hill, 1971.
    \bibitem{DiVNe}
        Dickhoss and Van Neck,
        \textit{Many-Body Theory Exposed},
        World Scientific, 2006
    \bibitem{shermorInv}
        Jack Sherman and Winifred J. Morrison,
        \textit{Adjustment of an Inverse Matrix Corresponding to a Change in One Element of a Given Matrix.} \\
        Annals of Mathematical Statistics 21, 1950. doi: \url{http://dx.doi.org/10.1214/aoms/1177729893}.
    \bibitem{linalgDavid}
        David C. Lay Chapter 3,
        \textit{Linear Algebra and Its Applications},
        Pearson, 2011, ISBN 978-0321835144.
\end{thebibliography}
\end{document}
