\documentclass[a4paper, hidelinks, 10pt]{article}\usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}
% \usepackage{libertine}
% \usepackage[libertine]{newtxmath}
\usepackage{charter}
\usepackage[expert, charter]{mathdesign}
\usepackage{listings}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{mathtools}
% \usepackage{amssymb}
\usepackage{amsmath}
\usepackage[margin=0.7in]{geometry}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{float}
\usepackage{caption}
\usepackage{scrextend}
\usepackage{ragged2e}
\usepackage{multirow}
\usepackage{array}
\usepackage{physics}
\usepackage{enumitem}
\usepackage{varwidth}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xifthen}
\usepackage{etoolbox}
\usepackage{xparse}
\usepackage{bbm}

%specific reused text
\newcommand{\mdate}{\today}
\newcommand{\mtitle}{FYS4411}
\newcommand{\mauthor}{Alfred Alocias Mariadason}
\newcommand{\massignn}{Project 2}

\pagestyle{fancy}
\fancyhf{}
% \fancyhead[LO, RE]{\small\leftmark}
\lhead{\small{\mtitle}}
\chead{\small{\massignn}}
\rhead{\small{\thesection}}
% \lfoot{}
\cfoot{\thepage}
% \rfoot{}

\patchcmd{\thebibliography}{\section*}{\section}{}{}

%renew title numbering
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\thesection.\Alph{subsection}}

%center title and subtitle
\let\oldsection\section
\renewcommand{\section}[1]{\centering \oldsection{{#1}} \justifying}
\let\oldsubsection\subsection
\renewcommand{\subsection}[1]{\centering \oldsubsection{{#1}} \justifying}

%set counter for algorithm
\newcommand{\algorithmautorefname}{algorithm}

%title settings
% \renewcommand{\headrulewidth}{0pt}
\renewcommand{\sectionautorefname}{section}
\renewcommand{\subsectionautorefname}{section}
\renewcommand{\subsubsectionautorefname}{section}
\renewcommand{\equationautorefname}{equation}
\renewcommand{\figureautorefname}{figure}
\renewcommand{\tableautorefname}{table}
\captionsetup{compatibility=false}

\patchcmd{\smallmatrix}{\thickspace}{\kern1.3em}{}{}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.3,0.3,0.3}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
        backgroundcolor=\color{backcolour},
        commentstyle=\color{codegreen},
        keywordstyle=\color{magenta},
        numberstyle=\tiny\color{codegray},
        stringstyle=\color{codepurple},
        basicstyle=\footnotesize,
        breakatwhitespace=false,
        breaklines=true,
        captionpos=b,
        keepspaces=true,
        numbers=left, 
        numbersep=4pt, 
        showspaces=false, 
        showstringspaces=false,
        showtabs=true, 
        tabsize=2
}
\lstset{style=mystyle}

\hypersetup{
    allcolors=black
}
\urlstyle{same}

\newcommand{\onefigure}[4]{
    \begin{figure}[H]
        \centering
        \textbf{{#1}}\\
        \includegraphics[scale=0.65]{{#2}}
        \caption{{#3}}
        \label{fig:#4}
    \end{figure}
    \justifying
} %one figure {filename}{caption}
\newcommand{\twofigure}[7]{
    \begin{figure}[H]
        \centering
        \begin{subfigure}[b!]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{{#1}}
            \caption{{#2}}
            \label{subfig:#3}
        \end{subfigure}
        \begin{subfigure}[b!]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{{#4}}
            \caption{{#5}}
            \label{subfig:#6}
        \end{subfigure}
        \caption{#7}
        \justify
    \end{figure}
} %two figure one-line {title}{file1}{caption1}{file2}{caption2}

\newcommand{\prtl}{\mathrm{\partial}} %reduce length of partial (less to write)
\newcommand{\md}{\mathrm{d}} %straight d for differential
\NewDocumentCommand{\prd}{m O{} O{}}{\frac{\prtl^{#3}{#2}}{\prtl{#1}^{#3}}}
\NewDocumentCommand{\mprd}{m O{} O{}}{\frac{\md^{#3}{#2}}{\md{#1}^{#3}}}
\NewDocumentCommand{\mixprd}{m O{} O{}}{\frac{\prtl^2{#1}}{\prtl{#2}\prtl{#3}}}
\newcommand{\vsp}{\vspace{0.2cm}} %small vertical space
\newcommand{\txtit}[1]{\textit{{#1}}} %italic text
\newcommand{\blds}[1]{\boldsymbol{{#1}}} % better bold in mathmode (from amsmath)
\newcommand{\bigO}{\mathcal{O}} %nice big O
\newcommand{\me}{\mathrm{e}} %straight e for exp
\newcommand{\mRe}[1]{\mathrm{Re}\left({#1}\right)}%nice real
\newcommand{\munit}[1]{\;\ensuremath{\, \mathrm{#1}}} %straight units in math
\newcommand{\Rarr}{\Rightarrow} %reduce lenght of Rightarrow (less to write)
\newcommand{\rarr}{\rightarrow} %reduce lenght of rightarrow (less to write)
\newcommand{\ecp}[1]{\left< {#1} \right>} %expected value
\newcommand{\urw}{\uparrow} % up arrow
\newcommand{\drw}{\downarrow} % up arrow
\newcommand{\pt}[1]{\textbf{\txtit{#1}}\justify}
\newcommand{\infint}{\int\limits^{\infty}_{-\infty}}
\newcommand{\oinfint}{\int\limits^{\infty}_0}
\newcommand{\sint}{\int\limits^{2\pi}_0\int\limits^{\pi}_0\oinfint}
\newcommand{\arcsinh}[1]{\text{arcsinh}\left(#1\right)}
% \newcommand{\I}{\scalebox{1.15}{$\mathds{1}$}}
\newcommand{\I}{\scalebox{1.1}{$\mathbbm{1}$}}
\newcommand{\veps}{\varepsilon} %\varepsilon is to long :P
\newcommand{\suml}[2]{\sum\limits_{#1=1}^{#2}}
\newcommand{\prodl}[2]{\prod\limits_{#1}^{#2}}
\newcommand{\adj}[1]{\text{adj}\left(#1\right)}

\newcommand{\fij}[3]{#1\left(#2\rarr#3\right)}
\newcommand{\ufij}[3]{#1_{#2\rarr#3}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\infoTabs}[2]{
    \begin{tabular}{crl}
        \multicolumn{3}{c}{\textbf{$\blds{N=#1}$}} \\
        \multicolumn{1}{c}{$R$} & \multicolumn{1}{c}{$E_0[au]$} &
        \multicolumn{1}{c}{$I$} \\
        \hline
        \input{#2}
    \end{tabular}
}

\newcommand{\infoTables}[7]{
    \begin{table}[H]
        \centering
        \textbf{Energies with $\blds{\omega=#1}$} \\
        \hrule \vspace{1.5pt} \hrule \vspace{0.7pt}
        \infoTabs{2}{#2}
        \infoTabs{6}{#3}
        \infoTabs{12}{#4}
        \infoTabs{20}{#5}
        \hrule \vspace{1.5pt} \hrule \vspace{0.7pt}
        \caption{#6}
        \justifying
        \label{tab:#7}
    \end{table}
}

\makeatletter
% define a macro \Autoref to allow multiple references to be passed to \autoref
\newcommand\Autoref[1]{\@first@ref#1,@}
\def\@throw@dot#1.#2@{#1}% discard everything after the dot
\def\@set@refname#1{%    % set \@refname to autoefname+s using \getrefbykeydefault
    \edef\@tmp{\getrefbykeydefault{#1}{anchor}{}}%
    \def\@refname{\@nameuse{\expandafter\@throw@dot\@tmp.@autorefname}s}%
}
\def\@first@ref#1,#2{%
  \ifx#2@\autoref{#1}\let\@nextref\@gobble% only one ref, revert to normal \autoref
  \else%
    \@set@refname{#1}%  set \@refname to autoref name
    \@refname~\ref{#1}% add autoefname and first reference
    \let\@nextref\@next@ref% push processing to \@next@ref
  \fi%
  \@nextref#2%
}
\def\@next@ref#1,#2{%
   \ifx#2@ and~\ref{#1}\let\@nextref\@gobble% at end: print and+\ref and stop
   \else, \ref{#1}% print  ,+\ref and continue
   \fi%
   \@nextref#2%
}
\makeatother

\newcommand{\gJ}{\exp(\frac{a}{\beta + \frac{1}{r_{ij}}})}
\newcommand{\gk}{\exp(\frac{a}{\beta + \frac{1}{r_{kj}}})}
\newcommand{\prodsecrule}[2]{2\left(\prd{x_k}[#2]\prd{x_k}[#1] +
\prd{y_k}[#2]\prd{y_k}[#1]\right) + {#2} \nabla^2_k {#1} + {#1} \nabla^2_k
{#2}}

\begin{document}
\thispagestyle{empty}
\begin{center} \vspace{1cm}
    \textbf{\Large{\mtitle\hspace{0.01pt} - Computational Physics II: Quantum
    Mechanical Systems}}\\ \vspace{0.5cm}
    \textbf{\large{\massignn\hspace{0.01pt} - Variatonal Monte Carlo Studies of
    Electronic Systems.}}\\ \vspace{1cm}
    \textbf{\large{\mauthor}}\\ \vspace{0.5cm}
    \large{\url{https://www.github.com/Oo1Insane1oO/FYS4411}} \\ \vspace{0.5cm}
    \Large{\mdate}\\ \vfill
\end{center}

\clearpage
\setcounter{page}{1}

\begin{center}
    \textbf{Abstract} \\
\end{center}

\section{INTRODUCTION}
\label{sec:introduction}
    Using the Variational Monte Carlo, this project aims to find and analyze
    quantities such as the ground state energy and single-particle densities of
    quantum dots for so-called closed shell systems.

    We use the usual approach by estimating expectation value of the ground
    state energy with the variational principle and minimizing. The algorithm
    used for the Monte Carlo method is the well known Metropolis algorithm.

    The reason for using a Monte Carlo method for minimizing the trial ground
    state energy is because the expectation value would in general be a
    multi-dimensional integral depending on the number of particles and number
    of parameters involved in the total wave function. Such an integral is not
    adequately solved by traditional methods(i.e Gaussian-quadrature).

    The desired result is that the Metropolis algorithm with importance
    sampling yields a better result both from a computational point of view.
    That is it finds a good estimate for the ground state energy efficiently
    without wasting to much time on the configuration space. The wave function
    only has small values in this large space meaning a homogeneous
    distribution of calculation points would yield a poor result, a
    non-homogeneous approach(such as with the Metropolis algorithm) would then,
    hopefully, gives a better result.

\section{THEORY}
\label{sec:theory}
    This section will explain the various theoretical approaches used in the
    project. All formulas, analytic expressions and derivations are given here.
    All the computational optimizations used are also given(see
    \Autoref{sub:optimization}).

\subsection{HERMITE POLYNOMIALS}
\label{sub:hermite_polynomials}
    Hermite polynomials $H(x)$ are solutions to the differential equation
        \begin{equation}
            \frac{\md^2 H}{\md x^2} -2x\frac{\md H}{\md x} + \left(\lambda
            -1\right)H = 0
            \label{eq:hermitediffeq}
        \end{equation}
    The polynomials fulfill the orthogonality relation 
        \begin{equation}
            \infint \me^{-x^2}H^2_n\md x = 2^nn!\sqrt{\pi}
            \label{eq:hermiteOrth}
        \end{equation}
    with the recurrence relation
        \begin{equation}
            H_{n+1} = 2xH_n - 2nH_{n-1}
            \label{eq:hermiteReq}
        \end{equation}
    and standardized relation
        \begin{equation}
            H_n = (-1)^n\me^{x^2}\prd{x}[][n]\me^{-x^2}
            \label{eq:hermiteExp}
        \end{equation}
    From \Autoref{eq:hermiteExp} one can find an expression for the derivative
    of the Hermite polynomial as
        \begin{equation}
            \prd{x}[H_n][m] = 2^mm!\frac{n!}{(n-m)!}H_{n-m}
            \label{eq:hermiteDer}
        \end{equation}

\subsection{HARMONIC OSCILLATOR}
\label{sub:harmonic_oscillator}
\subsubsection{Cartesian Coordinates}
\label{ssub:Cartesian Coordinates}
    The harmonic oscillator system in $2$ dimensions and in natural units is
    given by the following Hamiltonian
        \begin{equation}
            \hat{H_0} = \frac{1}{2}\sum^N_{i=1}\left(-\nabla^2_i + \omega^2
            r^2_i\right)
            \label{eq:cartHarmOsc}
        \end{equation}
    The wave functions in this case is then:
        \begin{equation}
            \phi_{n_x,n_y}(x,y) =
            AH_{n_x}(\sqrt{\omega}x)H_{n_y}(\sqrt{\omega}y)
            \exp(-\frac{\omega}{2}(x^2+y^2))
            \label{eq:cartUarmOscWavef}
        \end{equation}
    where $H_n$ is a Hermite polynomial of order $n$ and $A$ is a normalization
    constant. The quantum numbers $n_x$ and $n_y$ go as $n_x,n_y=0,1,2\dots$.
    While $\omega$ is the oscillator frequency. \\
    The energies is 
        \begin{equation}
            E = \hbar\omega\left(n_x + n_y + 1\right)
            \label{eq:cartHarmOscE}
        \end{equation}

\subsection{FOURIER TRANSFORMATION}
\label{sub:fourier_transformation}
    Given an integrable function $f:\mathbb{R}\rarr\mathbb{C}$ the Fourier
    transform to variable $x$ is defined to be
        \begin{equation}
            \tilde{f}(k) = \frac{1}{\sqrt{2\pi}}\infint f(x)\me^{ixs} \md x
            \label{eq:FourierTransform}
        \end{equation}
    with $x,s\in\mathbb{R}$. \\
    The inverse transformation is given as
        \begin{equation}
            f(x) = \frac{1}{\sqrt{2\pi}}\infint \tilde{f}(k)\me^{-isx} \md s
            \label{eq:invFourier}
        \end{equation}

\subsection{GRADIENT DESCENT}
\label{sub:gradient_descent}
    The method of gradient descent aims to minimize a function $f$ with respect
    to its $n$ parameters $\alpha_1,\dots,\alpha_n$. The idea is to blindly
    follow a portion of the gradient of the function giving the following
    estimate for the minimum
        \begin{equation}
            \begin{pmatrix}
                \alpha^{\text{new}}_1 \\
                \vdots \\
                \alpha^{\text{new}}_n \\
            \end{pmatrix} = 
            \begin{pmatrix}
                \alpha^{\text{old}}_1 \\
                \vdots \\
                \alpha^{\text{old}}_n \\
            \end{pmatrix} - \gamma
            \begin{pmatrix}
                \prd{x^{\text{old}}_1}[f] \\
                \vdots \\
                \prd{x^{\text{old}}_n}[f]
            \end{pmatrix}
            \label{eq:stepepestupdate}
        \end{equation}
    with $\gamma$ being the step-size determining the how much(portion) of the
    gradient to follow. \\
    One may have constant step-size or use for instance the Barzilai-Borwein
    method which updates the step-size by
        \begin{equation}
            \gamma_j = \frac{\left(\vec{\alpha}_j -
            \vec{\alpha}_{j-1}\right)^T\left(\nabla_nf(\vec{\alpha}_j) -
            \nabla_nf(\vec{\alpha}_{j-1})\right)}{\abs{\nabla_nf(\vec{\alpha}_j)
            - \nabla_nf(\vec{\alpha}_{j-1})}^2}
            \label{eq:barzilaiborwein}
        \end{equation}
    where $j$ indicates the step (new for $j$ and old for $j-1$). \\
    So we need either just one initial guess for the minimum, or two in case of
    \Autoref{eq:barzilaiborwein}.

\subsection{CONJUGATE GRADIENT METHOD}
\label{sub:conjugate_gradient_method}
    Suppose we have a linear system defined as
        \begin{equation}
            A\vec{x} = \vec{b}
            \label{eq:cglinsys}
        \end{equation}
    where $A$ is a $nxn$ real, symmetric and positive definite matrix while
    $\vec{x}$ and $\vec{b}$ are a non-zero real vectors. \\
    Define now the conjugate directions of two vectors
    $\vec{x}_i,\vec{x}_j:\mathbb{R}\rarr\mathbb{R}$ giving the constraint
        \begin{equation}
            \vec{x}_i^{T}A\vec{x}_j = \vec{0}
            \label{eq:cgconstr}
        \end{equation}
    This means that in the iterative process of finding $\vec{x}$ we perform
    searches within the conjugate directions of $\vec{x}$. \\
    Since the conjugate constraint(\Autoref{eq:cgconstr}) is defined by an
    inner product within the space $A$ is defined, the two vectors $\vec{x}_i$
    and $\vec{x}_j$ are orthogonal. If we now define a span $P$
        \begin{equation}
            P = \left\{\vec{p}_1,\dots,\vec{p}_n\right\}
            \label{eq:cgSpan}
        \end{equation}
    consisting of $n$ mutually orthogonal conjugate directions $\vec{p}_i$, we
    have a basis for $\mathbb{R}^n$. Expanding the solution $x_{i+1}$ to
    \Autoref{eq:cglinsys} in the mentioned basis
        \begin{equation}
            \vec{x} = \suml{i}{n} C_i\vec{p}_i
            \label{eq:cgnewstepbasisexpan}
        \end{equation}
    the linear system can be rewritten as
        \begin{equation}
            A\vec{x} = \suml{i}{n} C_iA\vec{p}_i = \vec{b}
            \label{eq:cgnewsystem}
        \end{equation}
    Giving the inner product
        \begin{equation}
            \vec{p}^T_k A\vec{x} = \suml{i}{n} C_i\vec{p}^T_kA\vec{p}_i
            \label{eq:cgnewinner}
        \end{equation}
    and we can define the coefficients $C_k$ as
        \begin{equation}
            C_k = \frac{\vec{p}^T_k\vec{b}}{\vec{p}^T_kA\vec{p}_k}
            \label{eq:cgNewCoeff}
        \end{equation}
    The problem at hand is then to choose a sequence of $n$ conjugate
    directions $P$ and compute the coefficients $C_k$.

\subsubsection{Iterative Method}
\label{ssub:iterative_method}
    The conjugate gradient method can in a similar manner be used on an
    iterative basis. We start with an initial guess $\vec{x}_0$ for the
    solutions and consider the linear system
        \begin{equation}
            A\vec{x} = \vec{b} - \vec{r}
            \label{eq:cgitlinsys}
        \end{equation}
    where $A,\vec{x}$ and $\vec{b}$ are defined as before and $\vec{r}$ is a
    so-called residual. \\
    Let $\vec{r}_k$ be the residual at the $k$-th step with a negative
    gradient. Using a similar approach as before we get that the conjugate
    direction $\vec{p}_{i+1}$ is
        \begin{equation}
            \vec{p}_{i+1} = \vec{r}_k -
            \frac{\vec{p}^T_kA\vec{r}_k}{\vec{p}^T_kA\vec{p}_k}\vec{p}_k
            \label{eq:cgitdirk}
        \end{equation}
    The iterative process is then to compute the directions and then solve the
    arising linear system by computing the coefficients as given in
    \Autoref{eq:cgNewCoeff}. \\
    The iterative process is then to compute
        \begin{equation}
            \vec{x}_{k+1} = \vec{x}_k + C_k\vec{p}_{k+1}
            \label{eq:cgitproc}
        \end{equation}
    with
        \begin{equation}
            C_k = \frac{\vec{p}^T_k\vec{r}_k}{\vec{p}^T_kA\vec{p}_k}
            \label{eq:cgitcoeff}
        \end{equation}
    The residual $\vec{r}_k$ at the $k$-th step is defined from
    \Autoref{eq:cgitlinsys} as
        \begin{equation}
            \vec{r}_k = \vec{b} - A\vec{x}_k
            \label{eq:cgitrk}
        \end{equation}

\subsection{LAPLACE EXPANSION}
\label{sub:laplace_expansion}
    Given a $n\times n$ matrix $M=[m_{ij}]$ and $i,j\in[1,\dots,n]$, then the
    determinant of $M$ is
        \begin{equation}
            \det(M) = \suml{j}{n}m_{ij}C_{ij}
            \label{eq:laplaceexp}
        \end{equation}
    where $C_{ij}$ is the $i,j$-element of the so-called cofactor matrix
    defined as the sub-matrix arising from removing the $i$-th row and $j$-th
    column from $M$. The cofactor matrix element is given as
        \begin{equation}
            C_{ij} = (-1)^{i+j}M_{ij}
            \label{eq:cofactor}
        \end{equation}
    where the indices $i$ and $j$ run from $1$ up to $n-1$ \cite{linalgDavid}.

\subsection{DERIVATIVE OF DETERMINANT}
\label{sub:derivative_of_determinant}
    We can express the derivative with respect to $t$ for a square matrix
    $A(t)$ of size $n$ which has a differentiable map $\md
    A\in\mathbb{R}\rarr\mathbb{R}^n$ as
        \begin{equation}
            \prd{t}[A(t)] = \Tr(\text{adj}\left(A(t)\right)\prd{t}[A(t)])
            \label{eq:JacobisDef}
        \end{equation}
    where $\Tr$ is the trace and $\text{adj}$ is the adjugate(transpose of
    cofactor matrix). This relation is known as Jacobi's formula
    \cite{linalgDavid}. \\
    We can specialize Jacobi's formula to
        \begin{equation}
            \prd{t}[A(t)] = \det(A)\Tr(A(t)^{-1}\prd{t}[A(t)])
            \label{eq:jacobiSpecial}
        \end{equation}
    if $A$ is invertible. This relation follows from the definition of the
    adjugate which gives
        \begin{equation}
            A\text{adj}\left(A\right) = \det(A)\I
            \label{eq:aadja}
        \end{equation}
    and in turn if $A$ is invertible
        \begin{equation}
            A^{-1} = \frac{1}{\det(A)}\text{adj}\left(A\right)
            \label{eq:aadjainv}
        \end{equation}
    (known as Cramers' rule). Here $\I$ is the $n\times n$ identity matrix.

\subsection{METROPOLIS-HASTINGS ALGORITHM}
\label{sub:metropolis_algorithm}
    The Metropolis algorithm bases itself on moves (also called transitions) as
    given in a Markov process(or Markov chain). Define a probability
    distribution function(PDF) $w_j(t)$ with a transition probability
    $\fij{w}{i}{j}$ which for a given time-step yields in the Markov formula
        \begin{equation}
            w_i(t+\veps) = \sum\limits_j \fij{w}{j}{i} w_j(t)
            \label{eq:MarkovFormula}
        \end{equation}
    The transition probability is defined with an acceptance probability
    distribution $\fij{A}{j}{i}$ and a proposed probability distribution
    $\fij{T}{j}{i}$ as
        \begin{equation}
            \fij{w}{j}{i} = \fij{A}{j}{i}\fij{T}{j}{i}
            \label{eq:wijdef}
        \end{equation}
    The acceptance $A$ is the probability for the move to be accepted and the
    proposal $T$ is different for each problem. \\
    In order for this transition chain to reach a desired convergence and
    reversibility we have the well known condition for detailed
    balance\cite{statmech}. This condition gives us that the probability
    distribution functions satisfy the following condition
        \begin{equation}
            w_i \ufij{T}{i}{j}\ufij{A}{i}{j} = w_j \ufij{T}{j}{i}\ufij{A}{j}{i}
            \Rarr \frac{w_i}{w_j} =
            \frac{\ufij{T}{j}{i}\ufij{A}{j}{i}}{\ufij{T}{i}{j}\ufij{A}{i}{j}}
            \label{eq:detailedBalance}
        \end{equation}
    We now need to choose an acceptance which fulfills
    \Autoref{eq:detailedBalance} and a common choice is the Metropolis
    condition
        \begin{equation}
            \ufij{A}{j}{i} = \min\left(1,
            \frac{w_i\ufij{T}{i}{j}}{w_j\ufij{T}{j}{i}}\right)
            \label{eq:MetropolisCondition}
        \end{equation}
    The Metropolis-Hastings algorithm is thus
        \begin{enumerate}[label=(\roman*)]
            \item Pick initial state $i$ at random.
            \item Pick proposed state at random in accordance to
                $\ufij{T}{j}{i}$.
            \item Accept state according to $\ufij{A}{j}{i}$.
            \item Jump to step (ii) until a specified number of states have
                been generated.
            \item Save the state $i$ and jump to step (ii).
        \end{enumerate}

\subsection{VARIATIONAL PRINCIPLE}
\label{sub:variational_principle}
    The variational principle states the following restriction on the ground
    state energy for a given symmetry
        \begin{equation}
            E_0 \leq \ecp{E[\Phi_T]} = \int \phi^{*}_T\hat{H}\phi_T\md\tau =
            \bra{\phi_T}\hat{H}\ket{\phi_T}
            \label{eq:HFvarE}
        \end{equation}
    that is the ground state energy $E_0$ is bounded by the expectation value
    of the trial energy.

\subsection{IMPORTANCE SAMPLING}
\label{sub:importance_sampling}
    In order to use the Metropolis algorithm as explained in
    \Autoref{sub:metropolis_algorithm}, we need to find the proposal
    probability distribution labeled $\ufij{T}{j}{i}$. This is what is known as
    importance sampling. \\
    This section will derive the importance sampling by using the Fokker-Planck
    equation for one particle
        \begin{equation}
            \prd{t}[P] = D\prd{x} \left(\prd{x} - F\right)P(x,t) 
            \label{eq:FokkerPlanc}
        \end{equation}
    where $F$ is a drift term and $D$ is a diffusion constant, and the Langevin
    equation
        \begin{equation}
            \prd{t}[x(t)] = DF(x(t)) + \eta
            \label{eq:Langevin}
        \end{equation}
    where $\eta$ is a Gaussian distributed random variable.

\subsubsection{Quantum Force}
\label{ssub:quantum_force}
    We will in this section give an outline of the derivation for the quantum
    force. See \cite{vmc} for more. \\
    Since we are working with a isotropic diffusion characterized by a
    time-dependant probability density our system must obey the summed total
    Fokker-Planck equation
        \begin{equation}
            \prd{t}[P] = \sum\limits_iD\prd{x_i}\left(\prd{x_i} -
            F_i\right)P(x,t)
            \label{eq:FokkerPlancTotal}
        \end{equation}
    where $F_i$ is now the i'th component of the drift velocity term(given by
    an external potential). Since the probability is assumed to be convergent,
    that is it converges to a stationary probability density the time
    dependence at this point is zero for all $i$. We also know that the drift
    should be of form $F=g(x)\prd{x}[P]$ giving
        \begin{equation}
            \prd{x_i}[P][2] = P\prd{P}[g]\left(\prd{x_i}[P]\right)^2 +
            Pg\prd{x_i}[P][2] + g\left(\prd{x_i}[P]\right)^2
            \label{eq:FokkerPlancStationary}
        \end{equation}
    Now we may use the condition for stationary density meaning the left hand
    side of \Autoref{eq:FokkerPlancStationary} must equal zero giving that
    $g=1/P$(only possibility the derivatives cancel). Inserting in $P=\psi_T$
    we get that the expression for the quantum force is
        \begin{equation}
            F_i = \frac{2}{\Psi_T}\nabla_i\Psi_T
            \label{eq:quantumForce}
        \end{equation}
    Notice that the quantum force is defined individually for each particle
    because of the assumption of of stationary probability density.

\subsubsection{Solution}
\label{ssub:Solution}
    Using Eulers method(Euler-Maruyama method\cite{numstoch}) on the Langevin
    equation(\Autoref{eq:Langevin}) one obtains the new positions
        \begin{equation}
            y = x + DF(x)\Delta t + \xi\sqrt{\Delta t}
            \label{eq:newPos}
        \end{equation}
    with $D=1/2$ in natural units due to the kinetic energy term and $\Delta t$
    is a time-step parameter. The random variable $\xi$ is within a Gaussian
    distribution of variance one and standard deviation zero. \\
    For the differential equation (\Autoref{eq:FokkerPlanc}) we insert in for
    the quantum force given in \Autoref{eq:quantumForce} and arrive at the
    following diffusion equation
        \begin{equation}
            \prd{t}[P] = -D\prd{r}[P][2]
            \label{eq:Pdiff}
        \end{equation}
    This equation is solved by using a Fourier transform in the spatial
    coordinate $r$ according to \Autoref{eq:FourierTransform} giving the equation
        \begin{equation}
            \prd{t}[\tilde{P}(s,t)] = -Ds^2\tilde{P}(s,t)
            \label{eq:PdiffFourier}
        \end{equation}
    with solution
        \begin{equation}
            \tilde{P}(s,\Delta t) = \tilde{P}(s,0) \me^{-Ds^2\Delta t}
            \label{eq:PdiffFourierSol}
        \end{equation}
    Initially the probability density is centered at $D\Delta tF(x)$, that is
    the drift term. This can be expressed terms of a Dirac-delta function,
    meaning 
        \begin{equation}
            P(y,x,0)=\delta(y-D\Delta tF(x))
            \label{eq:P0}
        \end{equation}
    where $y$ is given in \Autoref{eq:newPos}. Making an inverse transformation
    as described in \Autoref{eq:invFourier} and solving the subsequent
    transcendental integral yields in
        \begin{equation}
            P(y,x,\Delta t) = \frac{1}{\sqrt{4\pi D\Delta
            t}}\exp(-\frac{(y-x-D\Delta tF(x))^2}{4D\Delta t})
        \end{equation}
    which gives us the acceptance
        \begin{equation}
            \ufij{A}{y}{x} = \min\left(1, \frac{\abs{\psi_T(y)}^2P(y,x,\Delta
            t)} {\abs{\psi_T(x)}^2P(x,y,\Delta t)}\right)
            \label{eq:acceptanceImportance}
        \end{equation}

\subsection{VMC}
\label{sub:vmc}
    This section will explain and derive the equations involved in the
    Variational Monte Carlo method\cite{vmc}. \\ 
    The whole section will assume that we have the following trial wave
    function, $\psi_T$
        \begin{equation}
            \psi_T(\vec{r}_1,\dots,\vec{r}_N) \equiv
            \det(\phi_1(\vec{r}_1,\alpha),\dots,\phi(\vec{r}_N,\alpha))
            \prod^N_{i<j} \exp(\frac{ar_{ij}}{1+\beta r_{ij}})
            \label{eq:psiT}
        \end{equation}
    with the $\vec{r}$'s being the position of the electrons and the $\phi$'s
    being the wave function to some known system(i.e harmonic oscillator). The
    position $r_{ij}$ is a relative distance $\abs{\vec{r}_i-\vec{r}_j}$ while
    $\alpha$ and $\beta$ are variational parameters and $a$ is a specific
    constant dependant of the total spin symmetry of electron $i$ and $j$ as
        \begin{equation}
            a = \left\{
                    \begin{aligned}
                        1, & \indent \text{anti-parallel spin} \\
                        \frac{1}{3}, & \indent \text{parallel spin}
                    \end{aligned}
                \right.
            \label{eq:a}
        \end{equation}
    This is also known as a Pade-Jastrow factor. \\
    We also define the total Hamiltonian of the system for the quantum dot case
    as
        \begin{equation}
            \hat{H} = \hat{H}_O + \hat{H}_I
            \label{eq:totalHamil}
        \end{equation}
    with $\hat{H}_O$ being the harmonic oscillator defined in
    \Autoref{eq:cartHarmOsc} and $\hat{H}_I$ being the Hamiltonian for the
    electron interactions(Coulomb interaction) defined as
        \begin{equation}
            \hat{H}_I = \sum_{i<j}\frac{1}{r_{ij}}
            \label{eq:hamilCou}
        \end{equation}
    Lastly, we work in natural units setting $\hbar=c=1$, and all the above
    equations(\Autoref{eq:psiT,eq:a,eq:totalHamil,eq:hamilCou}) also assume
    natural units.

\subsubsection{Expectation Value and Local Energy}
\label{ssub:expectation_value_and_local_energy}
    Given the Hamiltonian \Autoref{eq:totalHamil} and a trial wave function
    $\Psi_T(R,\Lambda)$ and using the variational principle, as given in
    \Autoref{eq:HFvarE} the upper bound for the ground state energy $E_0$ if
    $H(r)$ is
        \begin{equation}
            E[\hat{H}(R,\Lambda)] \leq \ecp{\hat{H}} =
            \frac{\bra{\Psi_T}\hat{H}\ket{\Psi_T}}{\braket{\Psi_T}}
            \label{eq:HupperBound}
        \end{equation}
    where $R=(r_1,\dots,r_N)$ is the positions to $N$ particles and
    $\Lambda=(\lambda,\dots,\lambda_M)$ are the $M$ variational parameters. \\
    Now we can expand the trial wave function $\Psi_T(R,\Lambda)$ in the
    orthonormal eigenstates of the Hamiltonian $\hat{H}$(which form a complete
    set)
        \begin{equation}
            \Psi_T(r) = \sum_ic_i\Psi_i(r)
            \label{eq:PsiTExpand}
        \end{equation}
    and the upper bound given in \Autoref{eq:HupperBound} is
        \begin{equation}
            E_0 \leq \frac{\sum\limits_{ij}c_ic^{*}_j
            \bra{\Psi_j}\hat{H}\ket{\Psi_i}} {\sum\limits_{ij}c_ic^{*}_j
            \bra{\Psi_j}\ket{\Psi_i}} = \frac{\sum\limits_n
            a^2_nE_n}{\sum\limits_n a^2_n}
            \label{eq:HupperBoundExpand}
        \end{equation}
    where the eigenequation for the Hamiltonian $\hat{H}\Psi_n=E_n\Psi_n$ was
    used. The expression given in \Autoref{eq:HupperBound} is the expectation
    value we evaluate in each variational step that is we choose $\alpha$
    according to some minimization algorithm and re-evaluate the expectation
    value. \\
    In order to introduce the transition probability as given in the Metropolis
    algorithm(see \Autoref{sub:metropolis_algorithm}) the expectation value,
    \Autoref{eq:HupperBoundExpand}, needs to be rewritten in terms of a PDF. We
    can define this as
        \begin{equation}
            P(R) \equiv \frac{\abs{\Psi_T(R)}^2}{\int\abs{\Psi_T(R)}^2\md R}
            \label{eq:PDFdef}
        \end{equation}
    Now we observe that if we define a quantity
        \begin{equation}
            E_L(R,\Lambda) \equiv
            \frac{1}{\Psi_T(R,\Lambda)}\hat{H}\Psi_T(R,\Lambda)
            \label{eq:ELdef}
        \end{equation}
    which is the so-called local energy. The expectation value given in
    \Autoref{eq:HupperBoundExpand} can be rewritten as
        \begin{equation}
            E[H] = \int P(R)E_L(R,\Lambda) \md R \approx
            \frac{1}{N}\sum\limits_{i=1}^N P(r_i,\Lambda)E_L(r_i,\Lambda)
            \label{eq:ecpRew}
        \end{equation}
    which is of the form given in \Autoref{eq:MarkovFormula} and $N$ is the
    number of states(or Monte Carlo cycles).


\subsection{Optimization}
\label{sub:optimization}
    In the method described in \Autoref{sub:vmc} the heavy load in terms of
    calculation lies within the calculation of the determinant ratio given in
    \Autoref{eq:acceptanceImportance,eq:quantumForce}. This section will derive
    an expression for these ratios in terms of computation time. We will in the
    whole section assume we only move one particle at a time in the Monte Carlo
    cycle.

\subsubsection{Determinant Ratio}
\label{ssub:determinant_ratio}
\newcommand{\rnew}{r^{\text{new}}}
\newcommand{\rold}{r^{\text{old}}}
    In the Metropolis algorithm we calculate a ratio of determinants in the
    Metropolis test. Starting by defining a Slater determinant Matrix $D$ with
    entries defined as
        \begin{equation}
            D_{ij} \equiv \phi_j({r_i})
            \label{eq:Ddef}
        \end{equation}
    where the $\phi$'s are defined as in \Autoref{sub:vmc}. \\
    In terms of cofactors $C_{ij}$ we have
        \begin{equation}
            \det(D) = \sum\limits^N_{j=1} D_{ij}C_{ij}
            \label{eq:Dcofac}
        \end{equation}
    If we now take into light the mentioned assumption about moving only one
    particle at a time the determinant given in \Autoref{eq:Ddef} only gets a
    change of one row. \\
    Defining the ratio as
        \begin{equation}
            R \equiv \frac{\det(D\left(x^{\text{new}}\right))}
            {\det(D\left(x^{\text{old}}\right))}
            \label{eq:Rdef}
        \end{equation}
    Using the fact that when moving the particle at position $r_i$ the
    cofactors remain unchanged ($C^{\text{new}}=C^{\text{old}}$) and inserting
    in \Autoref{eq:Dcofac} into \Autoref{eq:Rdef} we have
        \begin{equation}
            R = \frac{\sum\limits^{N}_{j=1} D_{ij}(\rnew)C_{ij}(\rold)}
            {\sum\limits^{N}_{j=1} D_{ij}(\rold)C_{ij}(\rold)}
            \label{eq:R1}
        \end{equation}
    Since the Slater is square(closed shell) and invertible(since the
    determinant is non-zero\cite{linalgDavid}), we have the following ratio
        \begin{equation}
            D^{-1}_{ji} = \frac{C_{ij}}{\det(D)}
            \label{eq:Ddagger}
        \end{equation}
    inserting this into \Autoref{eq:R1} we get
        \begin{equation}
            R = \frac{\suml{j}{N}D_{ij}(\rnew)D^{-1}_{ji}(\rold)\abs{D}}
            {\suml{j}{N}D_{ij}(\rold)D^{-1}_{ji}(\rold)\abs{D}}
            \label{eq:Rrat}
        \end{equation}
    We also have
        \begin{equation}
            \suml{k}{N}D_{ik}D^{-1}_{kj} = \delta_{ij}
            \label{eq:Dorth}
        \end{equation}
    from the definition of the inverse, meaning the denominator in
    \Autoref{eq:Rrat} is equal to $1$ and the ratio is finally(with
    \Autoref{eq:Ddef} inserted)
        \begin{equation}
            R = \suml{j}{N}\phi_j(\rnew_i)\phi^{-1}_{ji}(\rold)
            \label{eq:Rfinal}
        \end{equation}
    We can follow a similar approach to find the ratio given in
    \Autoref{eq:quantumForce} for the quantum force. The expression is simply
        \begin{equation}
            \frac{\nabla_i\abs{D(r_i)}}{\abs{D(r_i)}} =
            \suml{j}{N}\nabla_i\phi_j(r_i)\phi^{-1}_{ji}(r)
            \label{eq:Rder}
        \end{equation}
    And a similarly for the Laplacian
        \begin{equation}
            \frac{\nabla^2_i\abs{D(r_i)}}{\abs{D(r_i)}} =
            \suml{j}{N}\nabla^2_i\phi_j(r_i)\phi^{-1}_{ji}(r)
            \label{eq:Rdersec}
        \end{equation}
    The only difference is that rather then looking at new and old positions we
    look at the change in the respective row as the derivative.

\subsubsection{Inverse of Matrix}
\label{ssub:inverse_of_matrix}
    In \Autoref{ssub:determinant_ratio} we derived a formula for calculating
    the ratio of determinants by the inverse of the old determinant when only
    one row is changed. This section gives a formula for updating the inverse
    of a matrix in that case. The formula is
        \begin{equation}
            D^{-1}_{kj}\left(\rnew\right) = 
                \left\{\begin{aligned}
                    &D^{-1}_{kj}\left(\rold\right) -
                    \frac{D^{-1}_{ik}\left(\rold\right)}{R}
                    \suml{l}{N}D_{il}\left(\rnew\right)D^{-1}_{lj}\left(\rold\right),&
                    j \neq i \\
                    &\frac{D^{-1}_{ik}\left(\rold\right)}{R}
                    \suml{l}{N}D_{il}\left(\rold\right)D^{-1}_{lj}\left(\rold\right),&
                    j = i
                \end{aligned}\right.
            \label{eq:InvUpdate}
        \end{equation}
    as described by Sherman and Morris \cite{vmc,shermorInv}. The factor $R$ is
    just the determinant ratio as described in \Autoref{eq:Rfinal}.

\subsubsection{Derivative Ratios}
\label{ssub:derivative_ratios}
    In the calculation of the local energy and quantum force we need to
    calculate the ratio between the derivatives of the trial wave function and
    the wave function it self. In this section we will derive a formula for
    these ratios based on the mentioned fact that we move only one particle at
    a time. \\
    We start with the first derivative and define the one body part to be
    $\psi_{\phi}$ and the correlation term(Jastrow) to be $\psi_J$ meaning
    $\psi_T=\psi_{\phi}\psi_J$. This gives the ratio
        \begin{equation}
            \frac{\nabla\psi_T}{\psi_T} = \frac{\nabla\psi_{\phi}}{\psi_{\phi}}
            + \frac{\nabla\psi_J}{\psi_J}
            \label{eq:optfirstdirrat}
        \end{equation}
    and for the second derivative we have
        \begin{equation}
            \frac{\nabla^2\psi_T}{\psi_T} =
            \frac{\nabla^2\psi_{\phi}}{\psi_{\phi}} +
            \frac{\nabla^2\psi_J}{\psi_J} +
            2\frac{\nabla\psi_{\phi}}{\psi_{\phi}} \cdot
            \frac{\nabla\psi_J}{\psi_J}
            \label{eq:optseconddirrat}
        \end{equation}
    For one-body part $\psi_{\phi}$ we know that the expression is just a
    determinant(from \Autoref{eq:psiT}). Using \Autoref{eq:Rder,eq:Rdersec} the
    expressions for the derivative ratios is
        \begin{equation}
            \begin{aligned}
                \frac{\nabla\psi_{\phi}}{\psi_{\phi}} &=
                \suml{j}{N}\nabla_i\phi_j(r_i)\psi_{\phi,ji}^{-1}(\vec{r}) \\
                \frac{\nabla^2\psi_T}{\psi_T} &=
                \suml{j}{N}\nabla^2_i\phi_j(r_i)\psi_{\phi,ji}^{-1}(\vec{r})
            \end{aligned}
            \label{eq:phiderrats}
        \end{equation}
    For the correlation factor $\psi_J$ we have an exponential form(see
    \Autoref{eq:psiT}). The first derivative ratio is thus
        \begin{equation}
            \left[\frac{\nabla\psi_J}{\psi_J}\right]_x = \suml{i}{k-1}
            \frac{x_i-x_k}{r_{ik}}\prd{r_{ik}}[f_{ik}] -
            \sum\limits_{i=k+1}^N\frac{x_k-x_i}{r_{ki}}\prd{r_{ki}}[f_{ki}]
            \label{eq:jastoptfirstder}
        \end{equation}
    due to the fact that the Jastrow-factor factor $g$ is only dependant of the
    relative distance $r_{ij}$ and is of exponential form and only the $N-1$
    terms differentiated survive. \\
    For the second derivative ratio we have similarly
        \begin{equation}
            \left[\frac{\nabla^2\psi_J}{\psi_J}\right]_x = \sum\limits_{i,j\neq
            k} \frac{\left(\vec{r_k} - \vec{r_i}\right) \left(\vec{r_k} -
            \vec{r_j}\right)}{r_{ki}r_{kj}}
            \prd{r_{ki}}[f_{ki}]\prd{r_{kj}}[f_{kj}] + \sum\limits_{j\neq k}
            \left(\prd{r_{kj}}[f_{kj}][2] +
            \frac{2}{r_{kj}}\prd{r_{kj}}[f_{kj}]\right)
            \label{eq:jastoptsecder}
        \end{equation}
    The factor $f_{ij}$ is just the function appearing in the exponent of the
    Jastrow factor. \\
    See \cite{vmc} for full derivation.

\subsection{STATISTICAL ANALYSIS}
\label{sub:statistical_analysis}
    Since Monte Carlo simulations can be considered to be computer experiments
    the resulting data can be analysed with the same statistical theory as one
    would with experimental data. This section will give a brief overview of
    some of the statistical concepts and explain the method of Blocking(for
    estimating the standard deviation).

\subsubsection{STANDARD DEVIATION, VARIANCE AND COVARIANCE}
\label{ssub:standard_deviation__variance_and_covariance}
    Given a PDF $P(x)$ the mean value, variance and covariance is
        \begin{align}
            \bar{x}_n &\equiv \frac{1}{n}\suml{k}{n} x_k \\
            \text{var}(x) &\equiv
            \frac{1}{n}\suml{k}{n}\left(x_k-\bar{x}_n\right)^2 \\
            \text{cov}(x) &\equiv \frac{1}{n}\sum\limits_{kl}
            \left(x_k-\bar{x}_n\right)\left(x_l-\bar{x}_n\right)
            \label{eq:statfinite}
        \end{align}
    for a finite size sample.

\subsubsection{Central Limit Theorem}
\label{ssub:central_limit_theorem}
    The central limit theorem states that given a PDF $P_{X_n}$ for a sample $X_n$
    the mean value can be expressed as
        \begin{equation}
            \lim\limits_{n\rarr\infty}p_{\bar{X}_n}(x) =
            \sqrt{\frac{n}{2\pi\sigma(X)^2}}
            \exp(-\frac{n(x-\bar{X}_n)^2}{2\sigma(X)^2})
            \label{eq:centrallimit}
        \end{equation}
    where $\sigma^2(X)$ is the variance of the sample.

\subsubsection{Statistical Error}
\label{ssub:statistical_error}
    The error in a sample is just the spread of the mean, i.e the variance of
    said sample. Set a finite size sample $X_n$ the error would be
        \begin{equation}
            \sigma^2_X = \text{var}\left(\bar{X}_n\right) =
            \frac{1}{n^2}\sum\limits_{ij}\text{cov}\left(X_i,X_j\right)
            \label{eq:error}
        \end{equation}
    Using the central limit theorem we can approximate the real mean as
        \begin{equation}
            \ecp{x_i} \approx \frac{1}{n}\suml{k}{n} x_k  = \bar{x}
            \label{eq:meanapprox}
        \end{equation}
    which gives the approximative covariance
        \begin{equation}
            \text{cov}\left(X_i,X_j\right) \approx
            \ecp{(x_i-\bar{x})(x_j-\bar{x})} = \frac{1}{n}\text{cov}(x)
            \label{eq:covapprox}
        \end{equation}
    and the error is thus
        \begin{equation}
            \sigma^2_X = \frac{1}{n}\text{cov}(x)
            \label{eq:errorfinal}
        \end{equation}
    We can split this equation in part giving
        \begin{align}
            \sigma^2_X &= \frac{1}{n}\text{var}(x) +
            \frac{1}{n}\left(\text{cov}(x) - \text{var}(x)\right) \nonumber \\
            &= \frac{1}{n^2}\suml{k}{n} \left(x_k - \bar{x}_n\right) +
            \frac{2}{n^2}\sum\limits_{k<l}
            \left(x_k-\bar{x}_n\right)\left(x_l-\bar{x}_n\right)
            \label{eq:errorSplit}
        \end{align}

\subsubsection{Autocorrelation Function}
\label{ssub:auto-correlation_function}
    Writing the second term in our error given in \Autoref{eq:errorSplit} (the
    so-called correlation term) as a partial sums gives
        \begin{equation}
            f_d = \frac{1}{n-d}\suml{k}{n-d}
            \left(x_k-\bar{x}_n\right)\left(x_{k+d}-\bar{x}_n\right)
            \label{eq:autocorrf}
        \end{equation}
    which this we can define the autocorrelation function $k_d$ as
        \begin{equation}
            k_d \equiv \frac{f_d}{\text{var}(x)}
            \label{eq:autocorrfinal}
        \end{equation}
    The sample error is now
        \begin{equation}
            \sigma^2 = \frac{\tau}{n}\text{var}(x)
            \label{eq:errorauto}
        \end{equation}
    with $\tau$ defined as the autocorrelation time
        \begin{equation}
            \tau \equiv 1 + 2\suml{d}{n-1}k_d
            \label{eq:autocorrtime}
        \end{equation}

    The method of blocking estimates the standard deviation given in

\section{SETUP}
\label{sec:setup}
\section{RESULTS}
\label{sec:results}
\section{DISCUSSION}
\label{sec:discussion}
\section{CONCLUSION}
\label{sec:conclusion}

\section{Appendix}
\label{sec:appendix}
\subsection{Analytic Expression for Local Energy}
\label{sub:analytic_expression_for_local_energy}
\subsubsection{Two Electron Case}
\label{ssub:Two Electron Case}
    We start by finding the local energy in the case with two electrons. The
    trial wave function is in this case(related to
    \Autoref{eq:cartUarmOscWavef}) using \Autoref{eq:psiT}
        \begin{equation}
            \psi_T(\vec{r}_1,\vec{r_2}) =
            A\exp(-\frac{\alpha\omega}{2}\left(r^2_1+r^2_2\right))
            \exp(\frac{ar_{12}}{1+\beta r_{12}})
            \label{eq:TECpsiT}
        \end{equation}
    Using the definition of the trial wave function, \Autoref{eq:ELdef} and the
    total Hamiltonian(\Autoref{eq:totalHamil}) the local energy with
    \Autoref{eq:ELdef} is
        \begin{equation}
            E_L = \frac{1}{\psi_T}\left(\hat{H}_O\psi_T +
            \hat{H}_I\psi_T\right)
            \label{eq:EL2}
        \end{equation}
    we solve the first part $\hat{H}_O\psi_T$
        \begin{equation}
            \hat{H}_O\psi_T = \frac{1}{2}\left(-\nabla^2_1 - \nabla^2_2 +
            \omega^2\left(r^2_1 + r^2_2\right)\right)\psi_T
            \label{eq:locF}
        \end{equation}
    Starting with the Laplacian for electron $1$ and solving the second
    derivative with respect to $x_1$ we have
        \begin{equation}
            \prd{x_1}[\psi_T][2] =
            A\exp(-\frac{\alpha\omega}{2}\left(r^2_2+y^2_1\right))
            \prd{x_1}[][2]\left[\exp(-\frac{\alpha\omega}{2}x^2_1 +
            \frac{a}{\beta + \frac{1}{r_{12}}})\right]
            \label{eq:derx1first}
        \end{equation}
    Starting with the first derivative in the exponential we get
        \begin{equation}
            \left.
                \begin{aligned}
                    \prd{x_1} \left[-\frac{\alpha\omega}{2}x^2_1\right] &=
                    -\alpha\omega x_1 \\
                    \prd{x_1} \left[\frac{a}{\beta + \frac{1}{r_{12}}}\right]
                    &= \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})^2}
                \end{aligned}
            \right\}\Rightarrow
            \prd{x_1}[\psi_T] = \left(-\alpha\omega x_1 +
            \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})^2}\right)\psi_T
            \label{eq:derx1first2}
        \end{equation}
    meaning \Autoref{eq:derx1first} is
        \begin{align}
            \prd{x_1}[\psi_T][2] &=
            A\exp(-\frac{\alpha\omega}{2}\left(r^2_2+y^2_1\right))
            \prd{x_1}\left[-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1 +
            \beta r_{12})^2}\right] \exp(-\frac{\alpha\omega}{2}x^2_1 +
            \frac{a}{\beta + \frac{1}{r_{12}}}) \nonumber \\
            &= \prd{x_1}\left[\left(-\alpha\omega x_1 +
            \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})}\right)\psi_T\right]
            \label{eq:derx1first3}
        \end{align}
    Using the product rule for differentiation and starting with the first
    expression we get that
        \begin{equation}
            \prd{x_1}\left[-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1 +
            \beta r_{12})^2}\right] = -\alpha\omega + \frac{a}{r_{12}(1 +
            \beta r_{12})^2} - \frac{a(x_1-x_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1+\beta r_{12})^3}
            \label{eq:derx1first4}
        \end{equation}
    giving
        \begin{align}
            \prd{x_1}[\psi_T][2] &= \psi_T\prd{x_1}\left[-\alpha\omega x_1 +
            \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})^2}\right] +
            \left(-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1 + \beta
            r_{12})^2}\right)\prd{x_1}[\psi_T] \nonumber \\ 
            &= \left[-\alpha\omega + \frac{a}{r_{12}(1+\beta r_{12})^2} -
            \frac{a(x_1-x_2)^2(1+3\beta r_{12})}{r^3_{12}(1 + \beta r_{12})^3}
            + \left(-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1+\beta
            r_{12})^2}\right)^2\right]\psi_T
            \label{eq:derx1first5}
        \end{align}
    The second derivative with respect to $y_1$ yields with a similar
    derivation
        \begin{equation}
            \prd{y_1}[\psi_T][2] = \left[-\alpha\omega +
            \frac{a}{r_{12}(1+\beta r_{12})^2} - \frac{a(y_1-y_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1 + \beta r_{12})^3} + \left(-\alpha\omega y_1 +
            \frac{a(y_1-y_2)}{r_{12}(1+\beta r_{12})^2}\right)^2\right]\psi_T
            \label{eq:dery1}
        \end{equation}
    The second derivatives with respect to $x_2$ and $y_2$, are derived in a
    similar manner, only we get a change in signs when differentiating
    $r_{12}$. This gives
        \begin{equation}
            \prd{x_2}[\psi_T][2] = \left[-\alpha\omega +
            \frac{a}{r_{12}(1+\beta r_{12})^2} - \frac{a(x_1-x_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1+\beta r_{12})^3} + \left(\alpha\omega x_2 +
            \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2}\right)^2\right]\psi_T
            \label{eq:derx2}
        \end{equation}
    and
        \begin{equation}
            \prd{y_2}[\psi_T][2] = \left[-\alpha\omega +
            \frac{a}{r_{12}(1+\beta r_{12})^2} - \frac{a(y_1-y_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1+\beta r_{12})^3} + \left(\alpha\omega y_2 +
            \frac{a(y_1-y_2)}{r_{12}(1+\beta r_{12})^2}\right)^2\right]\psi_T
            \label{eq:dery2}
        \end{equation}
    gathering \Autoref{eq:derx1first5,eq:dery1,eq:derx2,eq:dery2} we get
        \begin{align}
            (\nabla^2_1 + \nabla^2_2)\psi_T &= \prd{x_1}[\psi_T][2] +
            \prd{y_1}[\psi_T][2] + \prd{x_2}[\psi_T][2] + \prd{y_2}[\psi_T][2]
            \nonumber \\
            &= \left[-4\alpha\omega + \frac{4a}{r_{12}(1+\beta r_{12})^2} -
            \frac{2a(1+3\beta r_{12})}{r_{12}(1+\beta r_{12})^3} +
            \alpha^2\omega^2\left(r^2_1 + r^2_2\right) - \frac{2a\alpha\omega
            r_{12}}{(1+\beta r_{12})^2} + \frac{2a^2}{(1+\beta
            r_{12})^4}\right]\psi_T \\
            &= \left[\alpha^2\omega^2\left(r^2_1 + r^2_2\right) - 4\alpha\omega
            - \frac{2a\alpha\omega r_{12}}{(1+\beta r_{12})^2} +
            \frac{2a}{(1+\beta r_{12})^2}\left(\frac{a}{(1+\beta r_{12})^2} +
            \frac{1}{r_{12}} - \frac{2\beta}{1+\beta
            r_{12}}\right)\right]\psi_T
            \label{eq:derLast}
        \end{align}
    and the local energy(\Autoref{eq:EL2}) is finally
        \begin{equation}
            E_L = \frac{1}{2}\omega^2\left(1 - \alpha^2\right)\left(r^2_1 +
            r^2_2\right) + 2\alpha\omega - \frac{a}{(1+\beta
            r_{12})^2}\left(\frac{a}{(1+\beta r_{12})^2} - \alpha\omega r_{12}
            + \frac{1}{r_{12}} - \frac{2\beta}{1+\beta r_{12}}\right) +
            \frac{1}{r_{12}}
            \label{eq:EL2Final}
        \end{equation}

\subsubsection{General Case}
\label{ssub:general_case}
    For the general case with $N$ electrons(still closed shell) the local
    energy is defined as in \Autoref{eq:EL2}, but with $\psi_T$ defined as in
    \Autoref{eq:psiT}. The Laplacian in this case would then be
        \begin{equation}
            \nabla^2_N = \suml{k}{N} \nabla^2_k = \suml{k}{N}
            \left(\prd{x_k}[][2] + \prd{y_k}[][2]\right)
            \label{eq:nablaN}
        \end{equation}
    that is a sum over the single-particle spacial Laplacians. \\
    As mentioned we derive an expression based on the fact that we only move
    one particle at a time. This means we can rewrite the derivative ratio
    given in the local energy by using the formulas from
    \Autoref{sub:optimization}. We see that we need the second derivatives to
    the single particle wave functions. The single particle wave function
    is(\Autoref{eq:cartUarmOscWavef})
        \begin{equation}
            \Phi_{kj} = \phi_{n_{x_j}n_{y_j}}(r_k,\alpha) =
            H_{n_{x_j}}\left(\sqrt{\alpha\omega}x_k\right)
            H_{n_{y_j}}\left(\sqrt{\alpha\omega}y_k\right)
            \exp(-\frac{\alpha\omega}{2}\left(x^2_k + y^2_k\right))
            \label{eq:Phikj}
        \end{equation}
    The expression to be solved for the $k$-th particle is thus
        \begin{equation}
            \begin{aligned}
                \nabla^2_k \Phi_{kj} &=
                H_{n_{y_j}}\left(\sqrt{\alpha\omega}y_k\right)
                \exp(-\frac{\alpha\omega}{2}y^2_k)
                \prd{x_k}[][2]\left(H_{n_{x_j}}\left(\sqrt{\alpha\omega}x_k\right)
                \exp(-\frac{\alpha\omega}{2}x^2_k)\right) \\ &+
                H_{n_{x_j}}\left(\sqrt{\alpha\omega}x_k\right)
                \exp(-\frac{\alpha\omega}{2}x^2_k)
                \prd{y_k}[][2]\left(H_{n_{y_j}}\left(\sqrt{\alpha\omega}y_k\right)
                \exp(-\frac{\alpha\omega}{2}y^2_k)\right)
            \end{aligned}
            \label{eq:nablakth}
        \end{equation}
    Solving the differential for $x_k$ and similarly $y_k$ by substituting
    $s=\sqrt{\alpha\omega}x_k=\sqrt{\alpha\omega}y_k$(respectively) and
    labeling $n_x,n_y\rarr n$ and defining
        \begin{equation}
            \begin{aligned}
                e(x_k) &\equiv \exp(-\frac{\alpha\omega}{2}x^2_k) \\
                e(y_k) &\equiv \exp(-\frac{\alpha\omega}{2}y^2_k)
            \end{aligned}
            \label{eq:e}
        \end{equation}
    Using the product rule for differentiation and \Autoref{eq:hermiteDer} for
    the derivative of the Hermite polynomials with the recursion relation given
    in \Autoref{eq:hermiteReq} for $H_n(s)$ gives
        \begin{align}
            \alpha\omega \prd{s}[][2]\left(H_n(s)e(s))\right) &=
            \alpha\omega\left(\prd{s}[H_n][2] - 2s\prd{s}[H_n] + \left(s^2 -
            1\right)H_n(s)\right)e(s) \nonumber \\
            &= \alpha\omega\left(4n(n-1)H_{n-2}(s) - 4nsH_{n-1}(s) +
            (s^2-1)H_n(s)\right)e(s) \nonumber \\
            &= \alpha\omega\left(4n(n-1)H_{n-2}(s) - 2n\left(H_n(s) +
            2(n-1)H_{n-2}(s)\right) + \left(s^2-1\right)H_n(s)\right)e(s)
            \nonumber \\
            &= \alpha\omega\left(s^2 - 1 - 2n\right)H_n(s)e(s)
            \label{eq:diffs2}
        \end{align}
    We have in the latter step used \Autoref{eq:hermiteReq} to find $H_{n-1}$.
    \\
    We reuse the notation and differentiation from \Autoref{eq:diffs2} and find
    the first derivatives to be
        \begin{align}
            \sqrt{\alpha\omega}\prd{s}\left(H_n(s)e(s)\right) &=
            \sqrt{\alpha\omega}\left(\prd{s}[H_n] - sH_n\right)e(s) \nonumber
            \\
            &= \sqrt{\alpha\omega}\left(2nH_{n-1} - sH_n\right)e(s) \nonumber
            \\
            &= \sqrt{\alpha\omega}\left(2n\frac{H_{n-1}}{H_n} - s\right)
            H_n(s)e(s)
            \label{eq:sfirstder}
        \end{align}
    The ratios for the one-body part are thus
        \begin{equation}
            \begin{aligned}
                \frac{\nabla\det(\Phi)}{\det(\Phi)} &= \suml{k,j}{N} \nabla_k
                \phi_j\left(\vec{r}_k,\alpha\right)
                \phi^{-1}_{jk}\left(\vec{r},\alpha\right) \\
                &= \suml{k,j}{N} \left(2\sqrt{\alpha\omega}
                \left(n_{x_j}\frac{H_{n_{x_j}-1}
                \left(\sqrt{\alpha\omega}x_k\right)}
                {H_{n_{x_j}}\left(\sqrt{\alpha\omega}x_k\right)}\right) -
                \alpha\omega x_k, 2\sqrt{\alpha\omega}
                \left(n_{y_j}\frac{H_{n_{y_j}-1}
                \left(\sqrt{\alpha\omega}y_k\right)}
                {H_{n_{y_j}}\left(\sqrt{\alpha\omega}y_k\right)}\right) -
                \alpha\omega y_k\right) \Phi_{kj}\Phi^{-1}_{jk}
            \end{aligned}
            \label{eq:detderfirrat}
        \end{equation}
    and the Laplacian
        \begin{equation}
            \begin{aligned}
                \frac{\nabla^2\det(\Phi)}{\det(\Phi)} &= \suml{k,j}{N} \nabla^2_k
                \phi_j\left(\vec{r}_k,\alpha\right)
                \phi^{-1}_{jk}\left(\vec{r},\alpha\right) \\
                &= \alpha\omega\suml{k,j}{N} \left(\alpha^2\omega^2r^2_k -
                2\left(n_{x_j} + n_{y_j} + 1\right)\right) \Phi_{kj}\Phi^{-1}_{jk}
            \end{aligned}
            \label{eq:detdersecrat}
        \end{equation}
    For the Jastrow-factor we need the derivatives to the function in the
    exponent. Label this $f_{ij}$ and find
        \begin{align}
            \prd{r_{ij}}[f_{ij}] &= \prd{r_{ij}}\left(\frac{a}{\beta +
            \frac{1}{r_{ij}}}\right) \nonumber \\
            &= \frac{a}{\left(1+\beta r_{ij}\right)^2}
            \label{eq:gfijder1}
        \end{align}
    and
        \begin{equation}
            \prd{r_{ij}}[f_{ij}][2] = -\frac{2a\beta}{\left(1+\beta
            r_{ij}\right)^3}
            \label{eq:gfijder2}
        \end{equation}
    Giving the first derivative and the Laplacian of the Jastrow-factor(with
    \Autoref{eq:jastoptfirstder})
        \begin{equation}
            \begin{aligned}
                \frac{\nabla g(\vec{r},\beta)}{g(\vec{r},\beta)} &=
                \suml{i}{k-1}
                \frac{a\left(\vec{r}_i-\vec{r}_k\right)}{r_{ik}\left(1+\beta
                r_{ik}\right)^2} - \sum\limits_{i=k+1}^N
                \frac{a\left(\vec{r}_k-\vec{r}_i\right)}{r_{ki}\left(1+\beta
                r_{ki}\right)^2} \\
                &= \sum\limits_{i\neq k} \frac{2a\left(\vec{r_i} -
                \vec{r_k}\right)}{r_{ik}\left(1+\beta r_{ik}\right)^2}
            \end{aligned}
            \label{eq:jastfirderrat}
        \end{equation}
    and the Laplacian is (with \Autoref{eq:jastoptsecder})
        \begin{equation}
            \begin{aligned}
                \frac{\nabla^2 g(\vec{r},\beta)}{g(\vec{r},\beta)} &=
                \sum\limits_{i,j\neq k}
                \frac{\vec{r}_{ki}\cdot\vec{r}_{kj}}{r_{ki}r_{kj}}
                \frac{a_{ki}a_{kj}}{\left(1+\beta r_{ki}\right)^2 \left(1+\beta
                r_{kj}\right)^2} + \sum_{j\neq k}
                \frac{a_{kj}}{r_{kj}\left(1+\beta r_{kj}\right)^2}
                \left(\frac{1}{r_{kj}} - \frac{2\beta}{1+\beta r_{kj}}\right)
            \end{aligned}
            \label{eq:jastsecderrat}
        \end{equation}
    and the local energy is thus
        \begin{align}
            E_L &= \frac{1}{2}\suml{k}{N} \left(\omega^2r^2_k -
            \frac{\nabla^2_k\psi_T}{\psi_T}\right) + \sum_{i<j}
            \frac{1}{r_{ij}} \nonumber \\
            &= \frac{1}{2}\suml{k}{N} \left(\omega^2r^2_k -
            \frac{\nabla^2_k\det(\Phi(\vec{r},\alpha))}{\det(\Phi(\vec{r},\alpha))}
            - \frac{\nabla^2_kg(\vec{r},\beta)}{g(\vec{r},\beta)} -
            2\frac{\nabla_k\det(\Phi(\vec{r},\alpha))}{\det(\Phi(\vec{r},\alpha))}
            \cdot \frac{\nabla_kg(\vec{r},\beta)}{g(\vec{r},\beta)}\right) +
            \sum_{i<j} \frac{1}{r_{ij}}
            \label{eq:ELNfin}
        \end{align}

\subsection{Analytic Expression for Hessen Matrix}
\label{ssub:analytic_expression_for_hessen_matrix}
\newcommand{\psinorm}{\int\psi_T^2\md\tau}
\newcommand{\psigender}[1]{\prd{c_{#1}}[\psi_T]}
\newcommand{\psialphder}{\prd{\alpha}[\psi_T]}
\newcommand{\psialphsecder}{\prd{\alpha}[\psi_T][2]}
\newcommand{\psibetder}{\prd{\beta}[\psi_T]}
\newcommand{\psibetsecder}{\prd{\beta}[\psi_T][2]}
\newcommand{\gsum}{\sum\limits_{i\neq j} \frac{a}{\left(\beta + \frac{1}{r_{ij}}\right)}}
    The Hessen matrix is needed in order to minimize with the conjugate
    gradient method as described in \Autoref{sub:conjugate_gradient_method}.
    The Hessen matrix in the case where we minimize the local energy with
    respect to $\alpha$ and $\beta$ is\cite{linalgDavid}
        \begin{equation}
            A =
                \begin{pmatrix}
                    \prd{\alpha}[\ecp{E_L}][2] &
                    \frac{\prtl^2\ecp{E_L}}{\prtl\alpha\prtl\beta} \\
                    \frac{\prtl^2\ecp{E_L}}{\prtl\beta\prtl\alpha} &
                    \prd{\beta}[\ecp{E_L}][2]\\
                \end{pmatrix}
            \label{eq:analytichessen}
        \end{equation}
    We solve the derivatives generally for a variatonal parameters $c_n$. The
    first derivative is thus
        \begin{align}
            \prd{c_n}[\ecp{E_L}] &=
            \prd{c_n}\left(\int\frac{\psi_T^2E_L}{\psinorm}\right) \nonumber \\
            &= \int\left(\frac{\psinorm\left(2\psi_TE_L\psigender{n} +
            \psi_T^2\prd{c_n}[E_L]\right) -
            2\psi_T^2E_L\int\psi_T\psigender{n}\md\tau}
            {\left(\psinorm\right)^2}\right) \md\tau \nonumber \\
            &= 2\ecp{\frac{E_L}{\psi_T}\psigender{n}} -
            2\ecp{E_L}\ecp{\frac{1}{\psi_T}\psigender{n}}
            \label{eq:deralpahecpEl}
        \end{align}
    In the last step the hermiticity of the Hamiltonian was used. We have also
    used the fact that the trial wave function $\psi_T$ is a real function. \\
    The second derivative elements is then
        \begin{align}
            \mixprd{\ecp{E_L}}[c_n][c_m] &= 2\left(\prd{c_m}
            \left(\ecp{\frac{E_L}{\psi_T}\psigender{n}}\right) -
            \ecp{E_L}\prd{c_m} \left(\ecp{\frac{1}{\psi_T}\psigender{n}}\right)
            - \ecp{\frac{1}{\psi_T}\psigender{n}} \prd{c_m}[\ecp{E_L}]\right)
            \nonumber \\
            &\begin{aligned}
                \hspace{0.04cm}=&\hspace{0.07cm} 2\left(\prd{c_m}
                \ecp{\frac{E_L}{\psi_T}\psigender{n}}\right) -
                \ecp{E_L}\prd{c_m}
                \left(\ecp{\frac{1}{\psi_T}\psigender{n}}\right) \\
                &- 4\ecp{\frac{1}{\psi_T}\psigender{n}}
                \left(\ecp{\frac{E_L}{\psi_T}\psigender{m}} -
                \ecp{E_L}\ecp{\frac{1}{\psi_T}\psigender{m}}\right)
            \end{aligned} \nonumber \\
            &\begin{aligned}
                \hspace{0.04cm}=&\hspace{0.07cm}
                2\left(\ecp{\frac{E_L}{\psi^2_T} \psigender{n} \psigender{m}} +
                \ecp{\frac{1}{\psi_T} \prd{c_m}[E_L] \psigender{n}}  +
                \ecp{\frac{E_L}{\psi_T} \mixprd{\psi_T}[c_n][c_m]} -
                2\ecp{\frac{E_L}{\psi_T} \psigender{n}} \ecp{\frac{1}{\psi_T}
                \psigender{m}} \right) \\
                &- 2\ecp{E_L} \left(\ecp{\frac{1}{\psi^2_T} \psigender{n}
                \psigender{m}} + \ecp{\frac{1}{\psi_T}
                \mixprd{\psi_T}[c_n][c_m]} - 2\ecp{\frac{1}{\psi_T}
                \psigender{n}} \ecp{\frac{1}{\psi_T} \psigender{m}}\right) \\
                &- 4\left(\ecp{\frac{1}{\psi_T} \psigender{n}} \ecp{\frac{E_L}{\psi_T}
                \psigender{m}} - \ecp{E_L} \ecp{\frac{1}{\psi_T}
                \psigender{n}} \ecp{\frac{1}{\psi_T} \psigender{m}}\right)
            \end{aligned} \nonumber \\
            &\begin{aligned}
                \hspace{0.04cm}=&\hspace{0.07cm} 2\left(\ecp{\frac{E_L}{\psi_T}
                \mixprd{\psi_T}[c_n][c_m]} - \ecp{E_L}\ecp{\frac{1}{\psi_T}
                \mixprd{\psi_T}[c_n][c_m]} + \ecp{\frac{E_L}{\psi^2_T}
                \psigender{n} \psigender{m}} - \ecp{E_L}\ecp{\frac{1}{\psi^2_T}
                \psigender{n} \psigender{m}}\right) \\
                &+ \ecp{E_L} \ecp{\frac{1}{\psi_T}\psigender{n}}
                \ecp{\frac{1}{\psi}\psigender{m}} - 2\left(
                \ecp{\frac{E_L}{\psi_T}\psigender{n}}
                \ecp{\frac{1}{\psi_T}\psigender{m}} + \ecp{\frac{E_L}{\psi_T}
                \psigender{m}} \ecp{\frac{1}{\psi_T} \psigender{n}}\right) \\
                &+ 2\ecp{\frac{1}{\psi_T} \psigender{n} \prd{c_m}[E_L]}
            \end{aligned}
            \label{eq:der2alpahecpElfir}
        \end{align}
\newcommand{\Hij}[3]{H_{{#2}_{{#1}_j}#3}\left(\sqrt{\alpha\omega}{#1}_i\right)}
\newcommand{\eri}{\exp(-\frac{\alpha\omega}{2}r^2_i)}
    For the derivative of the wavefunction, we find the elements individually
    by using Jacobi's formula as described in
    \Autoref{sub:derivative_of_determinant}. We see that we need to first find
    the derivative to the individual elements in $\Phi$. These are found by
    using the product rule, but we also see that we need the derivative to the
    individual Hermite polynomials. These are simply (with the  chain rule)
        \begin{align}
            \prd{x_i}\Hij{x}{n}{} &=
            \frac{x_i}{2}\sqrt{\frac{\omega}{\alpha}}\prd{x_i}[H_{n_{x_j}}]
            \nonumber \\
            &= n_{x_j}x_i\sqrt{\frac{\omega}{\alpha}}\Hij{x}{n}{-1} \nonumber
            \\
            &= \frac{n_{x_j}}{2\alpha}\left(\sqrt{\alpha} +
            2x_i\sqrt{\omega}\left(n_{x_j}-1\right)
            \frac{\Hij{x}{n}{-2}}{\Hij{x}{n}{}}\right)\Hij{x}{n}{}
            \label{eq:phideralph1x}
        \end{align}
    The same differentiation is valid for $H_{n_{y_j}}$ as well. In the latter
    step we used \Autoref{eq:hermiteReq}. The first derivative of $\Phi_{ij}$
    with respect to $\alpha$ is thus
        \begin{align}
            \prd{\alpha}[\Phi_{ij}] &= \prd{\alpha} \left(\Hij{x}{n}{}\Hij{y}{n}{}
            \exp(-\frac{\alpha\omega}{2}r^2_i)\right) \nonumber \\
            &= \left(\Hij{y}{n}{}\prd{\alpha}\Hij{x}{n}{} +
            \Hij{x}{n}{}\prd{\alpha}\Hij{y}{n}{} - \frac{\omega
            r^2_i}{2}\right)\eri \nonumber \\
            &= \frac{1}{2}\left(\Theta_{ij}(\alpha,x) + \Theta_{ij}(\alpha,y) -
            \omega r^2_i\right)\Phi_{ij}
            \label{eq:phideralpha1}
        \end{align}
    with
        \begin{equation}
            \Theta_{ij}(\alpha,x) \equiv
            \frac{n_{x_j}}{\alpha}\left(\sqrt{\alpha} +
            2x_i\sqrt{\omega}\left(n_{x_j}-1\right)
            \frac{\Hij{x}{n}{-2}}{\Hij{x}{n}{}}\right)
            \label{eq:Thetadef}
        \end{equation}
    Using Jacobi's formula the first derivative with respect to $\alpha$ of
    $\psi_T$ is
        \begin{align}
            \prd{\alpha}[\psi_T] &= g\left(\vec{r},\beta\right)
            \prd{\alpha}\left(\det(\Phi)\right) \nonumber \\
            &= g\left(\vec{r},\beta\right) \Tr(\adj{\Phi}\prd{\alpha}[\Phi])
            \nonumber \\
            &= 
            g\left(\vec{r},\beta\right)
            \det(\Phi)\suml{i,j}{N} \Phi^{-1}_{ij}\prd{\alpha}[\Phi_{ji}]
            \nonumber \\
            &= \psi_T \suml{i,j}{N} \Phi^{-1}_{ij}
            \Phi_{ji}\left(\left(\Theta_{ji}(\alpha,x) + \Theta_{ji}(\alpha,y)\right) -
            \omega r^2_j\right)
            \label{eq:psideralpha}
        \end{align}
    where we have used \Autoref{eq:aadja} and defined $\Theta$ to be the vector
    of element as defined in \Autoref{eq:Thetadef} \\
    The second derivative of $\psi_T$ with respect to $\alpha$ we need the
    second derivative of the Hermite polynomials. These are
        \begin{align}
            \prd{\alpha}[][2]\Hij{x}{n}{} &= \prd{\alpha} \left(n_{x_j}x_i
            \sqrt{\frac{\omega}{\alpha}}\Hij{x}{n}{-1}\right)
            \nonumber \\
            &= n_{x_j}\left(n_{x_j}-1\right)x^2_i
            \frac{\omega}{\alpha}\Hij{x}{n}{-2} -\frac{n_{x_j}x_i}{2}
            \sqrt{\frac{\omega}{\alpha^3}}\Hij{x}{n}{-1} \nonumber \\
            &= \frac{n_{x_j}}{\alpha} \left(\left(n_{x_j}-1\right)x_i
            \left(\omega - \frac{1}{4}\sqrt{\frac{\omega}{\alpha}}\right)
            \frac{\Hij{x}{n}{-2}}{\Hij{x}{n}{}} -
            \frac{n_{x_j}}{4\alpha^2}\right)\Hij{x}{n}{}
            \label{eq:Hderalphasec}
        \end{align}
    The derivation is the same for $H_{n_{y_j}}$. The second derivative of
    $\Phi_{ij}$ is thus
        \begin{align}
            \prd{\alpha}[\phi_{ij}][2] &= \prd{\alpha}\left[
                \left(\Hij{y}{n}{}\prd{\alpha}\Hij{x}{n}{} +
                \Hij{x}{n}{}\prd{\alpha}\Hij{y}{n}{} - \frac{\omega
                r^2_i}{2}\right)\eri\right] \nonumber \\
            &\hspace{-0.04cm}\begin{aligned}
                &= \left(2\prd{\alpha}[\Hij{x}{n}{}]\prd{\alpha}[\Hij{y}{n}{}]
                \Hij{y}{n}{}\prd{\alpha}[\Hij{x}{n}{}][2] \right.\\ 
                &\hspace{0.6cm} + \left.
                \Hij{x}{n}{}\prd{\alpha}[\Hij{y}{n}{}][2] -
                \prd{\alpha}[\Phi_{ij}] \frac{\omega r^2_i}{2}\right)
                \exp(-\frac{\alpha\omega}{2}r^2_i)
            \end{aligned}
            &=
            \label{eq:phideralphasec}
        \end{align}
    The derivative with respect to $\beta$ follows a similar approach as in
    \Autoref{ssub:general_case} giving
        \begin{align}
            \prd{\beta}[\psi_T] &= \det(\Phi\left(\vec{r},\alpha\right))
            \prd{\beta}\left(\prod_{i<j}\gJ\right) \nonumber \\
            &= \det(\Phi\left(\vec{r},\alpha\right))
            g\left(\vec{r},\beta\right)
            \prd{\beta}\left(\sum_{i<j}\frac{a}{\beta +
            \frac{1}{r_{ij}}}\right) \nonumber \\
            &= -\psi_T \sum_{i\neq j} \frac{a}{\left(\beta +
            \frac{1}{r_{ij}}\right)^2}
            \label{eq:psiderbeta}
        \end{align}
    The second derivative is thus
        \begin{align}
            \prd{\beta}[\psi_T][2] &= \prd{\beta}\left(-\psi_T \sum_{i\neq j}
            \frac{a}{\left(\beta + \frac{1}{r_{ij}}\right)^2}\right) \nonumber
            \\
            &= \left(\left(\sum\limits_{i\neq j}\frac{a}{\left(\beta +
            \frac{1}{r_{ij}}\right)^2}\right)^2 + \sum_{i\neq
            j}\frac{2a}{\left(\beta + \frac{1}{r_{ij}}\right)^3} \right)\psi_T
            \label{eq:psiderbetasec}
        \end{align}
    where we simply used the product rule for differentiation. \\
    The mixed second derivative is found with
    \Autoref{eq:psideralpha,eq:psiderbeta} giving
        \begin{align}
            \frac{\prtl^2\psi_T}{\prtl\alpha\beta} &=
            \prd{\beta}\left(-\psi_T\frac{\omega}{2}\suml{i}{N}r^2_i\right)
            \nonumber \\
            &= \psi_T\frac{\omega}{2}\suml{i}{N}r^2_i\sum_{i\neq
            j}\frac{1}{\left(\beta + \frac{1}{r_{ij}}\right)^2}
            \label{eq:psideralpbetsec}
        \end{align}
    The final expression for the elements in the Hessen matrix is thus
        \begin{equation}
            \begin{aligned}
                \prd{\alpha}[\ecp{E_L}][2] &= \omega^2 \left(\ecp{E_LR^2} -
                \ecp{E_L}\ecp{R^2} + \ecp{E_L}\ecp{R}^2 -
                \ecp{E_LR}\ecp{R}\right) - \omega\ecp{R\prd{\alpha}[E_L]} \\
                \prd{\beta}[\ecp{E_L}][2] &= 4\left(\ecp{E_LB^2_2} -
                \ecp{E_L}\ecp{B^2_2}\right) + \ecp{E_L}\ecp{B_2}^2 -
                \ecp{E_LB_2}\ecp{B_2} - 2\ecp{B_2\prd{\beta}[E_L]} \\
                \mixprd{\ecp{E_L}}[\alpha][\beta] &=
                \omega\left(2\left(\ecp{E_LRB_2} - \ecp{E_L}\ecp{RB_2}\right) +
                \frac{1}{2}\ecp{E_L}\ecp{R}\ecp{B_2} - \ecp{E_LR}\ecp{B_2} -
                \ecp{E_LB_2}\ecp{R} - \ecp{R\prd{\beta}[E_L]}\right)
            \end{aligned}
            \label{eq:HessenElemFin}
        \end{equation}
    where 
        \begin{equation}
            \begin{aligned}
                R &\equiv \suml{i}{N}r^2_i \\ 
                B_n &\equiv \sum\limits_{i\neq j} \frac{a}{\left(\beta +
                \frac{1}{r_{ij}}\right)^n} 
            \end{aligned}
            \hspace{0.5cm}
            \begin{aligned}
                \prd{\alpha}[\psi_T] &= -\frac{\omega}{2}R\psi_T  \\
                \prd{\beta}[\psi_T] &= -B_2\psi_T 
            \end{aligned}
            \hspace{0.5cm}
            \begin{aligned}
                \prd{\alpha}[\psi_T][2] &= \frac{\omega^2}{4}R^2\psi_T \\
                \prd{\beta}[\psi_T][2] &= \left(B^2_2 + 2B_3\right)\psi_T
            \end{aligned}
            \hspace{0.5cm}
            \begin{aligned}
                \frac{\prtl^2\psi_T}{\prtl\alpha\prtl\beta} =
                \frac{\omega}{2}RB_2\psi_T
            \end{aligned}
            \label{eq:RBdef}
        \end{equation}






\begin{thebibliography}{}
    \bibitem{GriffQuan}
        David J. Griffiths Chapters 2, 4 and 7,
        \textit{Introduction to Quantum Mechanics Second Edition},
        Pearson, 2005, ISBN 0-13-111892-7.
    \bibitem{basicMB}
        Morten Hjort-Jensen,
        \textit{Computational Physics: Hartree-Fock methods and introduction to Many-Body Theory.} \\
        \url{https://www.github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/pub/basicMB/pdf}, 2017
    \bibitem{vmc}
        Morten Hjort-Jensen,
        \textit{Computational Physics: Variational Monte Carlo methods.} \\
        \url{https://www.github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/pub/vmc/pdf}, 2017
    \bibitem{taut}
        M. Taut, 
        \textit{Two Electrons in an External Oscillator Potential; Particular analytic solutions of a Coulomb Correlation Problem},
        Physical Review A 48, 3561, 1993.
    \bibitem{NeOr}
        Negele and Orland,
        \textit{Quantum Many-Particle Systems},
        Addison-Wesley.
    \bibitem{FeWa}
        Fetter and Walecka,
        \textit{Quantum Theory of Many-Particle Systems},
        Mcgraw-Hill, 1971.
    \bibitem{DiVNe}
        Dickhoss and Van Neck,
        \textit{Many-Body Theory Exposed},
        World Scientific, 2006
    \bibitem{shermorInv}
        Jack Sherman and Winifred J. Morrison,
        \textit{Adjustment of an Inverse Matrix Corresponding to a Change in One Element of a Given Matrix.} \\
        Annals of Mathematical Statistics 21, 1950. doi: \url{http://dx.doi.org/10.1214/aoms/1177729893}.
    \bibitem{linalgDavid}
        David C. Lay Chapter 3,
        \textit{Linear Algebra and Its Applications},
        Pearson, 2011, ISBN 978-0321835144.
    \bibitem{numstoch}
        Peter E. Kloeden and Eckhard Platen,
        \textit{Numerical Solution of Stochastic Differential Equations},
        Springer, 1992, ISBN 978-3-662-12616-5.
    \bibitem{statmech}
        J.P. Sethna,
        \textit{Statistical Mechanics: Entropy, Order Parameters and Complexity},
        \url{http://pages.physics.cornell.edu/sethna/StatMech/EntropyOrderParametersComplexity.pdf}.
\end{thebibliography}
\end{document}
