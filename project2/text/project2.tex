\documentclass[a4paper, hidelinks, 10pt]{article}
\usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}
% \usepackage{libertine}
% \usepackage[libertine]{newtxmath}
\usepackage{charter}
\usepackage[expert, charter]{mathdesign}
\usepackage{listings}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{mathtools}
% \usepackage{amssymb}
\usepackage[margin=0.7in]{geometry}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{float}
\usepackage{caption}
\usepackage{scrextend}
\usepackage{ragged2e}
\usepackage{multirow}
\usepackage{array}
\usepackage{physics}
\usepackage{enumitem}
\usepackage{varwidth}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{dsfont}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xifthen}
\usepackage{etoolbox}
\usepackage{xparse}

%specific reused text
\newcommand{\mdate}{\today}
\newcommand{\mtitle}{FYS4411}
\newcommand{\mauthor}{Alfred Alocias Mariadason}
\newcommand{\massignn}{Project 2}

\pagestyle{fancy}
\fancyhf{}
% \fancyhead[LO, RE]{\small\leftmark}
\lhead{\small{\mtitle}}
\chead{\small{\massignn}}
\rhead{\small{\thesection}}
% \lfoot{}
\cfoot{\thepage}
% \rfoot{}

\patchcmd{\thebibliography}{\section*}{\section}{}{}

%renew title numbering
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\thesection.\Alph{subsection}}

%center title and subtitle
\let\oldsection\section
\renewcommand{\section}[1]{\centering \oldsection{{#1}} \justifying}
\let\oldsubsection\subsection
\renewcommand{\subsection}[1]{\centering \oldsubsection{{#1}} \justifying}

%set counter for algorithm
\newcommand{\algorithmautorefname}{algorithm}

%title settings
% \renewcommand{\headrulewidth}{0pt}
\renewcommand{\sectionautorefname}{section}
\renewcommand{\subsectionautorefname}{section}
\renewcommand{\subsubsectionautorefname}{section}
\renewcommand{\equationautorefname}{equation}
\renewcommand{\figureautorefname}{figure}
\renewcommand{\tableautorefname}{table}
\captionsetup{compatibility=false}

\patchcmd{\smallmatrix}{\thickspace}{\kern1.3em}{}{}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.3,0.3,0.3}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
        backgroundcolor=\color{backcolour},
        commentstyle=\color{codegreen},
        keywordstyle=\color{magenta},
        numberstyle=\tiny\color{codegray},
        stringstyle=\color{codepurple},
        basicstyle=\footnotesize,
        breakatwhitespace=false,
        breaklines=true,
        captionpos=b,
        keepspaces=true,
        numbers=left, 
        numbersep=4pt, 
        showspaces=false, 
        showstringspaces=false,
        showtabs=true, 
        tabsize=2
}
\lstset{style=mystyle}

\hypersetup{
    allcolors=black
}
\urlstyle{same}

\newcommand{\onefigure}[4]{
    \begin{figure}[H]
        \centering
        \textbf{{#1}}\\
        \includegraphics[scale=0.65]{{#2}}
        \caption{{#3}}
        \label{fig:#4}
    \end{figure}
    \justifying
} %one figure {filename}{caption}
\newcommand{\twofigure}[7]{
    \begin{figure}[H]
        \centering
        \begin{subfigure}[b!]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{{#1}}
            \caption{{#2}}
            \label{subfig:#3}
        \end{subfigure}
        \begin{subfigure}[b!]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{{#4}}
            \caption{{#5}}
            \label{subfig:#6}
        \end{subfigure}
        \caption{#7}
        \justify
    \end{figure}
} %two figure one-line {title}{file1}{caption1}{file2}{caption2}

\newcommand{\prtl}{\mathrm{\partial}} %reduce length of partial (less to write)
\newcommand{\md}{\mathrm{d}} %straight d for differential
\NewDocumentCommand{\prd}{m O{} O{}}{\frac{\prtl^{#3}{#2}}{\prtl{#1}^{#3}}}
\NewDocumentCommand{\mprd}{m O{} O{}}{\frac{\md^{#3}{#2}}{\md{#1}^{#3}}}
\newcommand{\vsp}{\vspace{0.2cm}} %small vertical space
\newcommand{\txtit}[1]{\textit{{#1}}} %italic text
\newcommand{\blds}[1]{\boldsymbol{{#1}}} % better bold in mathmode (from amsmath)
\newcommand{\bigO}{\mathcal{O}} %nice big O
\newcommand{\me}{\mathrm{e}} %straight e for exp
\newcommand{\mRe}[1]{\mathrm{Re}\left({#1}\right)}%nice real
\newcommand{\munit}[1]{\;\ensuremath{\, \mathrm{#1}}} %straight units in math
\newcommand{\Rarr}{\Rightarrow} %reduce lenght of Rightarrow (less to write)
\newcommand{\rarr}{\rightarrow} %reduce lenght of rightarrow (less to write)
\newcommand{\ecp}[1]{\left< {#1} \right>} %expected value
\newcommand{\urw}{\uparrow} % up arrow
\newcommand{\drw}{\downarrow} % up arrow
\newcommand{\pt}[1]{\textbf{\txtit{#1}}\justify}
\newcommand{\infint}{\int\limits^{\infty}_{-\infty}}
\newcommand{\oinfint}{\int\limits^{\infty}_0}
\newcommand{\sint}{\int\limits^{2\pi}_0\int\limits^{\pi}_0\oinfint}
\newcommand{\arcsinh}[1]{\text{arcsinh}\left(#1\right)}
\newcommand{\I}{\scalebox{1.2}{$\mathds{1}$}}
\newcommand{\veps}{\varepsilon} %\varepsilon is to long :P

\newcommand{\fij}[3]{#1\left(#2\rarr#3\right)}
\newcommand{\ufij}[3]{#1_{#2\rarr#3}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\infoTabs}[2]{
    \begin{tabular}{crl}
        \multicolumn{3}{c}{\textbf{$\blds{N=#1}$}} \\
        \multicolumn{1}{c}{$R$} & \multicolumn{1}{c}{$E_0[au]$} &
        \multicolumn{1}{c}{$I$} \\
        \hline
        \input{#2}
    \end{tabular}
}

\newcommand{\infoTables}[7]{
    \begin{table}[H]
        \centering
        \textbf{Energies with $\blds{\omega=#1}$} \\
        \hrule \vspace{1.5pt} \hrule \vspace{0.7pt}
        \infoTabs{2}{#2}
        \infoTabs{6}{#3}
        \infoTabs{12}{#4}
        \infoTabs{20}{#5}
        \hrule \vspace{1.5pt} \hrule \vspace{0.7pt}
        \caption{#6}
        \justifying
        \label{tab:#7}
    \end{table}
}

\makeatletter
% define a macro \Autoref to allow multiple references to be passed to \autoref
\newcommand\Autoref[1]{\@first@ref#1,@}
\def\@throw@dot#1.#2@{#1}% discard everything after the dot
\def\@set@refname#1{%    % set \@refname to autoefname+s using \getrefbykeydefault
    \edef\@tmp{\getrefbykeydefault{#1}{anchor}{}}%
    \def\@refname{\@nameuse{\expandafter\@throw@dot\@tmp.@autorefname}s}%
}
\def\@first@ref#1,#2{%
  \ifx#2@\autoref{#1}\let\@nextref\@gobble% only one ref, revert to normal \autoref
  \else%
    \@set@refname{#1}%  set \@refname to autoref name
    \@refname~\ref{#1}% add autoefname and first reference
    \let\@nextref\@next@ref% push processing to \@next@ref
  \fi%
  \@nextref#2%
}
\def\@next@ref#1,#2{%
   \ifx#2@ and~\ref{#1}\let\@nextref\@gobble% at end: print and+\ref and stop
   \else, \ref{#1}% print  ,+\ref and continue
   \fi%
   \@nextref#2%
}
\makeatother

\begin{document}
\thispagestyle{empty}
\begin{center} \vspace{1cm}
    \textbf{\Large{\mtitle\hspace{0.01pt} - Computational Physics II: Quantum
    Mechanical Systems}}\\ \vspace{0.5cm}
    \textbf{\large{\massignn\hspace{0.01pt} - Variatonal Monte Carlo
    Methods}}\\ \vspace{1cm}
    \textbf{\large{\mauthor}}\\ \vspace{0.5cm}
    \large{\url{https://www.github.com/Oo1Insane1oO/FYS4411}} \\ \vspace{0.5cm}
    \Large{\mdate}\\ \vfill
\end{center}

\clearpage
\setcounter{page}{1}

\begin{center}
    \textbf{Abstract} \\
\end{center}

\section{INTRODUCTION}
\label{sec:introduction}
    Using the Variational Monte Carlo, this project aims to find and analyze
    quantities such as the ground state energy and single-particle densities of
    quantum dots for so-called closed shell systems.

    We use the usual approach by estimating expectation value of the ground
    state energy with the variational principle and minimizing. The algorithm
    used for the Monte Carlo method is the well known Metropolis algorithm.

    The reason for using a Monte Carlo method for minimizing the trial ground
    state energy is because the expectation value would in general be a
    multi-dimensional integral depending on the number of particles and number
    of parameters involved in the total wave function. Such an integral is not
    adequately solved by traditional methods(i.e Gaussian-quadrature).

    The desired result is that the Metropolis algorithm with importance
    sampling yields a better result both from a computational point of view.
    That is it finds a good estimate for the ground state energy efficiently
    without wasting to much time on the configuration space. The wave function
    only has small values in this large space meaning a homogeneous
    distribution of calculation points would yield a poor result, a
    non-homogeneous approach(such as with the Metropolis algorithm) would then,
    hopefully, gives a better result.

\section{THEORY}
\label{sec:theory}
    >>INSERT DESCRIPTION<<

\subsection{HERMITE POLYNOMIALS}
\label{sub:hermite_polynomials}
    Hermite polynomials $H(x)$ are solutions to the differential equation
        \begin{equation}
            \frac{\md^2 H}{\md x^2} -2x\frac{\md H}{\md x} + \left(\lambda
            -1\right)H = 0
            \label{eq:hermitediffeq}
        \end{equation}
    The polynomials fulfill the orthogonality relation 
        \begin{equation}
            \infint \me^{-x^2}H^2_n\md x = 2^nn!\sqrt{\pi}
            \label{eq:hermiteOrth}
        \end{equation}
    with the recurrence relation
        \begin{equation}
            H_{n+1} = H_n - 2nH_{n-1}
            \label{eq:hermiteReq}
        \end{equation}

\subsection{HARMONIC OSCILLATOR}
\label{sub:harmonic_oscillator}
\subsubsection{Cartesian Coordinates}
\label{ssub:Cartesian Coordinates}
    The harmonic oscillator system in $2$ dimensions and in natural units is
    given by the following Hamiltonian
        \begin{equation}
            \hat{H_0} = \frac{1}{2}\sum^N_{i=1}\left(-\nabla^2_i + \omega^2
            r^2_i\right)
            \label{eq:cartHarmOsc}
        \end{equation}
    The wave functions in this case is then:
        \begin{equation}
            \phi_{n_x,n_y}(x,y) =
            AH_{n_x}(\sqrt{\omega}x)H_{n_y}(\sqrt{\omega}y)
            \exp(-\frac{\omega}{2}(x^2+y^2))
            \label{eq:cartUarmOscWavef}
        \end{equation}
    where $H_n$ is a hermite polynomial of order $n$ and $A$ is a normalization
    constant. The quantum numbers $n_x$ and $n_y$ go as $n_x,n_y=0,1,2\dots$.
    While $\omega$ is the oscillator frequency. \\
    The energies is 
        \begin{equation}
            E = \hbar\omega\left(n_x + n_y + 1\right)
            \label{eq:cartHarmOscE}
        \end{equation}

\subsection{METROPOLIS-HASTINGS ALGORITHM}
\label{sub:metropolis_algorithm}
    The Metropolis algorithm bases itself on moves (also called transitions) as
    given in a Markov process(or Markov chain). Define a probability
    distribution function(PDF) $w_j(t)$ with a transition probability
    $\fij{w}{i}{j}$ which for a given time-step yields in the Markov formula
        \begin{equation}
            w_i(t+\veps) = \sum\limits_j \fij{w}{j}{i} w_j(t)
            \label{eq:MarkovFormula}
        \end{equation}
    The transition probability is defined with an acceptance probability
    distribution $\fij{A}{j}{i}$ and a proposed probability distribution
    $\fij{T}{j}{i}$ as
        \begin{equation}
            \fij{w}{j}{i} = \fij{A}{j}{i}\fij{T}{j}{i}
            \label{eq:wijdef}
        \end{equation}
    The acceptance $A$ is the probability for the move to be accepted and the
    proposal $T$ is different for each problem. \\
    In order for this transition chain to reach a desired convergence and
    reversibility we have the well known condition for detailed balance
    >>INSERT REF<<. This condition gives us that the probability distribution
    functions satisfy the following condition
        \begin{equation}
            w_i \ufij{T}{i}{j}\ufij{A}{i}{j} = w_j \ufij{T}{j}{i}\ufij{A}{j}{i}
            \Rarr \frac{w_i}{w_j} =
            \frac{\ufij{T}{j}{i}\ufij{A}{j}{i}}{\ufij{T}{i}{j}\ufij{A}{i}{j}}
            \label{eq:detailedBalance}
        \end{equation}
    We now need to choose an acceptance which fulfills
    \Autoref{eq:detailedBalance} and a common choice is the Metropolis
    condition
        \begin{equation}
            \ufij{A}{j}{i} = \min\left(1,
            \frac{w_i\ufij{T}{i}{j}}{w_j\ufij{T}{j}{i}}\right)
            \label{eq:MetropolisCondition}
        \end{equation}
    The Metropolis-Hastings algorithm is thus
        \begin{enumerate}[label=(\roman*)]
            \item Pick initial state $i$ at random.
            \item Pick proposed state at random in accordance to
                $\ufij{T}{j}{i}$.
            \item Accept state according to $\ufij{A}{j}{i}$.
            \item Jump to step (ii) until a specified number of states have
                been generated.
            \item Save the state $i$ and jump to step (ii).
        \end{enumerate}

\subsection{VARIATIONAL PRINCIPLE}
\label{sub:variational_principle}
    The variational principle states the following restriction on the ground
    state energy for a given symmetry
        \begin{equation}
            E_0 \leq \ecp{E[\Phi_T]} = \int \phi^{*}_T\hat{H}\phi_T\md\tau =
            \bra{\phi_T}\hat{H}\ket{\phi_T}
            \label{eq:HFvarE}
        \end{equation}
    that is the ground state energy $E_0$ is bounded by the expectation value
    of the trial energy.

\subsection{IMPORTANCE SAMPLING}
\label{sub:importance_sampling}
    In order to use the Metropolis algorithm as explained in
    \Autoref{sub:metropolis_algorithm}, we need to find the proposal
    probability distribution labeled $\ufij{T}{j}{i}$. This is what is known as
    importance sampling. \\
    This section will derive the importance sampling by using the Fokker-Planck
    equation for one particle
        \begin{equation}
            \prd{t}[P] = D\prd{x} \left(\prd{x} - F\right)P(x,t) 
            \label{eq:FokkerPlanc}
        \end{equation}
    where $F$ is a drift term and $D$ is a diffusion constant, and the Langevin
    equation
        \begin{equation}
            \prd{t}[x(t)] = DF(x(t)) + \eta
            \label{eq:Langevin}
        \end{equation}
    where $\eta$ is a Gaussian distributed random variable.

\subsubsection{Quantum Force}
\label{ssub:quantum_force}
    Since we are working with a isotropic diffusion characterized by a
    time-dependant probability density our system must obey the summed total
    Fokker-Planck equation
        \begin{equation}
            \prd{t}[P] = \sum\limits_iD\prd{x_i}\left(\prd{x_i} -
            F_i\right)P(x,t)
            \label{eq:FokkerPlancTotal}
        \end{equation}
    where $F_i$ is now the i'th component of the drift velocity term(given by
    an external potential). Since the probability is assumed to be convergent,
    that is it converges to a stationary probability density the time
    dependence at this point is zero for all $i$. We also know that the drift
    should be of form $F=g(x)\prd{x}[P]$ giving
        \begin{equation}
            \prd{x_i}[P][2] = P\prd{P}[g]\left(\prd{x_i}[P]\right)^2 +
            Pg\prd{x_i}[P][2] + g\left(\prd{x_i}[P]\right)^2
            \label{eq:FokkerPlancStationary}
        \end{equation}
    Now we may use the condition for stationary density meaning the left hand
    side of \Autoref{eq:FokkerPlancStationary} must equal zero giving that
    $g=1/P$(only possibility the derivatives cancel). Inserting in
    $P=\psi_T$(see >>INSERT REF<<) we get that the expression for the quantum
    force is
        \begin{equation}
            F = \frac{2}{\Psi_T}\nabla\Psi_T
            \label{eq:quantumForce}
        \end{equation}

\subsubsection{Solution}
\label{ssub:Solution}
    Using Eulers method(Euler-Maruyama method>>INSERT REF<<) on the Langevin
    equation(\Autoref{eq:Langevin}) one obtains the new positions
        \begin{equation}
            y = x + DF(x)\Delta t + \xi\sqrt{\Delta t}
            \label{eq:newPos}
        \end{equation}
    with $D=1/2$ in natural units due to the kinetic energy term and $\Delta t$
    is a time-step parameter. The random variable $\xi$ is within a Gaussian
    distribution of variance one and standard deviation zero. \\
    Using the force obtained in \Autoref{ssub:quantum_force} given in
    \Autoref{eq:quantumForce} the solution to the Fokker-Planck equations are
    given by the Greens function
        \begin{equation}
            G(y,x,\Delta t) = \frac{1}{\left(4\pi D\Delta
            t\right)^{\frac{3N}{2}}} \exp(-\frac{\left(x - \xi\sqrt{\Delta
            t}\right)^2}{4D\Delta t})
            \label{eq:FokkerLangevinGreens}
        \end{equation}
    which gives us the acceptance
        \begin{equation}
            \ufij{A}{y}{x} = \min\left(1, \frac{\abs{\psi_T(y)}^2G(y,x,\Delta
            t)}{\abs{\psi_T(x)}^2G(x,y,\Delta t)}\right)
            \label{eq:acceptanceImportance}
        \end{equation}

\subsection{VMC}
\label{sub:vmc}
    This section will explain and derive the equations involved in the
    Variational Monte Carlo method. The whole section will assume that we have
    the following trial wave function, $\psi_T$
        \begin{equation}
            \psi_T(\vec{r}_1,\dots,\vec{r}_N) \equiv
            \det(\phi_1(\vec{r}_1,\alpha),\dots,\phi(\vec{r}_N,\alpha))
            \prod^N_{i<j} \exp(\frac{ar_{ij}}{1+\beta r_{ij}})
            \label{eq:psiT}
        \end{equation}
    with the $\vec{r}$'s being the position of the electrons and the $\phi$'s
    being the wave function to some known system(i.e harmonic oscillator). The
    position $r_{ij}$ is a relative distance $\abs{\vec{r}_i-\vec{r}_j}$ while
    $\alpha$ and $\beta$ are variational parameters and $a$ is a specific
    constant dependant of the total spin symmetry of electron $i$ and $j$ as
        \begin{equation}
            a = \left\{
                    \begin{aligned}
                        1, & \indent \text{anti-parallel spin} \\
                        \frac{1}{3}, & \indent \text{parallel spin}
                    \end{aligned}
                \right.
            \label{eq:a}
        \end{equation}
    This is also known as a Pade-Jastrow factor. \\
    We also define the total Hamiltonian of the system for the quantum dot case
    as
        \begin{equation}
            \hat{H} = \hat{H}_O + \hat{H}_I
            \label{eq:totalHamil}
        \end{equation}
    with $\hat{H}_O$ being the harmonic oscillator defined in
    \Autoref{eq:cartHarmOsc} and $\hat{H}_I$ being the Hamiltonian for the
    electron interactions(Coulomb interaction) defined as
        \begin{equation}
            \hat{H}_I = \sum_{i<j}\frac{1}{r_{ij}}
            \label{eq:hamilCou}
        \end{equation}
    Lastly, we work in natural units setting $\hbar=c=1$, and all the above
    equations(\Autoref{eq:psiT,eq:a,eq:totalHamil,eq:hamilCou}) also assume
    natural units.

\subsubsection{Expectation Value and Local Energy}
\label{ssub:EXPECTATION VALUE AND LOCAL ENERGY}
    Given the Hamiltonian \Autoref{eq:totalHamil} and a trial wave function
    $\Psi_T(R,\Lambda)$ and using the variational principle, as given in
    \Autoref{eq:HFvarE} the upper bound for the ground state energy $E_0$ if
    $H(r)$ is
        \begin{equation}
            E[\hat{H}(R,\Lambda)] \leq \ecp{\hat{H}} =
            \frac{\bra{\Psi_T}\hat{H}\ket{\Psi_T}}{\braket{\Psi_T}}
            \label{eq:HupperBound}
        \end{equation}
    where $R=(r_1,\dots,r_N)$ is the positions to $N$ particles and
    $\Lambda=(\lambda,\dots,\lambda_M)$ are the $M$ variational parameters. \\
    Now we can expand the trial wave function $\Psi_T(R,\Lambda)$ in the
    orthonormal eigenstates of the Hamiltonian $\hat{H}$(which form a complete
    set)
        \begin{equation}
            \Psi_T(r) = \sum_ic_i\Psi_i(r)
            \label{eq:PsiTExpand}
        \end{equation}
    and the upper bound given in \Autoref{eq:HupperBound} is
        \begin{equation}
            E_0 \leq \frac{\sum\limits_{ij}c_ic^{*}_j
            \bra{\Psi_j}\hat{H}\ket{\Psi_i}} {\sum\limits_{ij}c_ic^{*}_j
            \bra{\Psi_j}\ket{\Psi_i}} = \frac{\sum\limits_n
            a^2_nE_n}{\sum\limits_n a^2_n}
            \label{eq:HupperBoundExpand}
        \end{equation}
    where the eigenequation for the Hamiltonian $\hat{H}\Psi_n=E_n\Psi_n$ was
    used. The expression given in \Autoref{eq:HupperBound} is the expectation
    value we evaluate in each variational step that is we choose $\alpha$
    according to some minimization algorithm and re-evaluate the expectation
    value. \\
    In order to introduce the transition probability as given in the Metropolis
    algorithm(see \Autoref{sub:metropolis_algorithm}) the expectation value,
    \Autoref{eq:HupperBoundExpand}, needs to be rewritten in terms of a PDF. We
    can define this as
        \begin{equation}
            P(R) \equiv \frac{\abs{\Psi_T(R)}^2}{\int\abs{\Psi_T(R)}^2\md R}
            \label{eq:PDFdef}
        \end{equation}
    Now we observe that if we define a quantity
        \begin{equation}
            E_L(R,\Lambda) \equiv
            \frac{1}{\Psi_T(R,\Lambda)}\hat{H}\Psi_T(R,\Lambda)
            \label{eq:ELdef}
        \end{equation}
    which is the so-called local energy. The expectation value given in
    \Autoref{eq:HupperBoundExpand} can be rewritten as
        \begin{equation}
            E[H] = \int P(R)E_L(R,\Lambda) \md R \approx
            \frac{1}{N}\sum\limits_{i=1}^N P(r_i,\Lambda)E_L(r_i,\Lambda)
            \label{eq:ecpRew}
        \end{equation}
    which is of the form given in \Autoref{eq:MarkovFormula} and $N$ is the
    number of states(or Monte Carlo cycles).

\subsubsection{Analytical Expression for Local Energy}
\label{ssub:ANALYTICAL EXPRESSION FOR LOCAL ENERGY}
    We use the Metropolis algorithm to find an estimate for the expectation
    value to the energy. In this expression we have a so-called local energy
    defined as
        \begin{equation}
            E_L = \frac{1}{\psi_T}\hat{H}\psi_T
            \label{eq:ELana}
        \end{equation}
    This expression shows up in the integrand as the multiplied function to the
    PDF which is used in the Metropolis algorithm.

\subsubsection{Two Electron Case}
\label{ssub:Two Electron Case}
    We start by finding the local energy in the case with two electrons. The
    trial wave function is in this case(related to
    \Autoref{eq:cartUarmOscWavef}) using \Autoref{eq:psiT}
        \begin{equation}
            \psi_T(\vec{r}_1,\vec{r_2}) =
            A\exp(-\frac{\alpha\omega}{2}\left(r^2_1+r^2_2\right))
            \exp(\frac{ar_{12}}{1+\beta r_{12}})
            \label{eq:TECpsiT}
        \end{equation}
    Using the definition of the trial wave function, \Autoref{eq:ELdef} and the
    total Hamiltonian(\Autoref{eq:totalHamil}) the local energy with
    \Autoref{eq:ELdef} is
        \begin{equation}
            E_L = \frac{1}{\psi_T}\left(\hat{H}_O\psi_T +
            \hat{H}_I\psi_T\right)
            \label{eq:EL2}
        \end{equation}
    we solve the first part $\hat{H}_O\psi_T$
        \begin{equation}
            \hat{H}_O\psi_T = \frac{1}{2}\left(-\nabla^2_1 - \nabla^2_2 +
            \omega^2\left(r^2_1 + r^2_2\right)\right)\psi_T
            \label{eq:locF}
        \end{equation}
    Starting with the Laplacian for electron $1$ and solving the second
    derivative with respect to $x_1$ we have
        \begin{equation}
            \prd{x_1}[\psi_T][2] =
            A\exp(-\frac{\alpha\omega}{2}\left(r^2_2+y^2_1\right))
            \prd{x_1}[][2]\left[\exp(-\frac{\alpha\omega}{2}x^2_1 +
            \frac{a}{\beta + \frac{1}{r_{12}}})\right]
            \label{eq:derx1first}
        \end{equation}
    Starting with the first derivative on the exponential we get
        \begin{equation}
            \left.
                \begin{aligned}
                    \prd{x_1} \left[-\frac{\alpha\omega}{2}x^2_1\right] &=
                    -\alpha\omega x_1 \\
                    \prd{x_1} \left[\frac{a}{\beta + \frac{1}{r_{12}}}\right]
                    &= \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})^2}
                \end{aligned}
            \right\}\Rightarrow
            \prd{x_1}[\psi_T] = \left(-\alpha\omega x_1 +
            \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})^2}\right)\psi_T
            \label{eq:derx1first2}
        \end{equation}
    meaning \Autoref{eq:derx1first} is
        \begin{align}
            \prd{x_1}[\psi_T][2] &=
            A\exp(-\frac{\alpha\omega}{2}\left(r^2_2+y^2_1\right))
            \prd{x_1}\left[-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1 +
            \beta r_{12})^2}\right] \exp(-\frac{\alpha\omega}{2}x^2_1 +
            \frac{a}{\beta + \frac{1}{r_{12}}}) \nonumber \\
            &= \prd{x_1}\left[\left(-\alpha\omega x_1 +
            \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})}\right)\psi_T\right]
            \label{eq:derx1first3}
        \end{align}
    Using the product rule for differentiation and starting with the first
    expression we get that
        \begin{equation}
            \prd{x_1}\left[-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1 +
            \beta r_{12})^2}\right] = -\alpha\omega + \frac{a}{r_{12}(1 +
            \beta r_{12})^2} - \frac{a(x_1-x_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1+\beta r_{12})^3}
            \label{eq:derx1first4}
        \end{equation}
    giving
        \begin{align}
            \prd{x_1}[\psi_T][2] &= \psi_T\prd{x_1}\left[-\alpha\omega x_1 +
            \frac{a(x_1-x_2)}{r_{12}(1 + \beta r_{12})^2}\right] +
            \left(-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1 + \beta
            r_{12})^2}\right)\prd{x_1}[\psi_T] \nonumber \\ 
            &= \left[-\alpha\omega + \frac{a}{(1+\beta r_{12})^2} -
            \frac{a(x_1-x_2)^2(1+3\beta r_{12})}{r^3_{12}(1 + \beta r_{12})^3}
            + \left(-\alpha\omega x_1 + \frac{a(x_1-x_2)}{r_{12}(1+\beta
            r_{12})^2}\right)^2\right]\psi_T
            \label{eq:derx1first5}
        \end{align}
    The second derivative with respect to $y_1$ yields with a similar
    derivation
        \begin{equation}
            \prd{y_1}[\psi_T][2] = \left[-\alpha\omega + \frac{a}{(1+\beta r_{12})^2} -
            \frac{a(y_1-y_2)^2(1+3\beta r_{12})}{r^3_{12}(1 + \beta r_{12})^3}
            + \left(-\alpha\omega y_1 + \frac{a(y_1-y_2)}{r_{12}(1+\beta
            r_{12})^2}\right)^2\right]\psi_T
            \label{eq:dery1}
        \end{equation}
    The second derivatives with respect to $x_2$ and $y_2$, are derived in a
    similar manner, only we get a change in signs when differentiating
    $r_{12}$. This gives
        \begin{equation}
            \prd{x_2}[\psi_T][2] = \left[-\alpha\omega +
            \frac{a}{r_{12}(1+\beta r_{12})^2} - \frac{a(x_1-x_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1+\beta r_{12})^3} + \left(\alpha\omega x_2 +
            \frac{a(x_1-x_2)}{r_{12}(1+\beta r_{12})^2}\right)^2\right]\psi_T
            \label{eq:derx2}
        \end{equation}
    and
        \begin{equation}
            \prd{y_2}[\psi_T][2] = \left[-\alpha\omega +
            \frac{a}{r_{12}(1+\beta r_{12})^2} - \frac{a(y_1-y_2)^2(1+3\beta
            r_{12})}{r^3_{12}(1+\beta r_{12})^3} + \left(\alpha\omega y_2 +
            \frac{a(y_1-y_2)}{r_{12}(1+\beta r_{12})^2}\right)^2\right]\psi_T
            \label{eq:dery2}
        \end{equation}
    gathering \Autoref{eq:derx1first5,eq:dery1,eq:derx2,eq:dery2} we get
        \begin{align}
            (\nabla^2_1 + \nabla^2_2)\psi_T &= \prd{x_1}[\psi_T][2] +
            \prd{y_1}[\psi_T][2] + \prd{x_2}[\psi_T][2] + \prd{y_2}[\psi_T][2]
            \nonumber \\
            &= \left[-4\alpha\omega + \frac{4a}{r_{12}(1+\beta r_{12})^2} -
            \frac{2a(1+3\beta r_{12})}{r_{12}(1+\beta r_{12})^3} +
            \alpha^2\omega^2\left(r^2_1 + r^2_2\right) - \frac{2a\alpha\omega
            r_{12}}{(1+\beta r_{12})^2} + \frac{2a^2}{(1+\beta
            r_{12})^4}\right]\psi_T \\
            &= \left[\alpha^2\omega^2\left(r^2_1 + r^2_2\right) - 4\alpha\omega
            - \frac{2a\alpha\omega r_{12}}{(1+\beta r_{12})^2} +
            \frac{2a}{(1+\beta r_{12})^2}\left(\frac{a}{(1+\beta r_{12})^2} +
            \frac{1}{r_{12}} - \frac{2\beta}{1+\beta
            r_{12}}\right)\right]\psi_T
            \label{eq:derLast}
        \end{align}
    and the local energy(\Autoref{eq:EL2}) is finally
        \begin{equation}
            E_L = \frac{1}{2}\omega^2\left(1 - \alpha^2\right)\left(r^2_1 +
            r^2_2\right) + 2\alpha\omega - \frac{a}{(1+\beta
            r_{12})^2}\left(\frac{a}{(1+\beta r_{12})^2} - \alpha\omega r_{12}
            + \frac{1}{r_{12}} - \frac{2\beta}{1+\beta r_{12}}\right) +
            \frac{1}{r_{12}}
            \label{eq:EL2Final}
        \end{equation}
    
%     Before we arrive at the final expression for the expectation value(which
%     the Metropolis algorithm will be applied on) we start by defining a trial
%     wave function $\psi_T$ as
%         \begin{equation}
%             \psi_T(\vec{r}_1,\dots,\vec{r}_N;\alpha_1,\dots)
%         \end{equation}
%     where the $r$'s are the position for the individual particles and the
%     $\alpha$'s are variational parameters, and use the variational principle to
%     arrive at
%         \begin{equation}
%             E_0 \leq \ecp{E(\alpha_1,\dots)} =
%             \frac{\int\psi^{*}_T\hat{H}\psi_T\md^N\vec{r}}
%             {\abs{\psi_T}^2\md^N\vec{r}}
%             \label{eq:vmcVarE}
%         \end{equation}
%     with $\hat{H}$ being the Hamiltonian and $\md^N\vec{r}$ is a short hand for
%     an integral over $\vec{r}_1,\dots,\vec{r}_N$.

\subsection{Optimization}
\label{sub:optimization}
    In the method described in \Autoref{sub:vmc} the heavy load in terms of
    calculation lies within the calculation of the determinant ratio >>REF
    HERE<<. This section will derive an expression for these ratios in terms of
    computation time. We will in the whole section assume we only move one
    particle at a time in the Monte Carlo cycle.

\subsubsection{Determinant Ratio}
\label{ssub:determinant_ratio}
\newcommand{\rnew}{r^{\text{new}}}
\newcommand{\rold}{r^{\text{old}}}
\newcommand{\suml}[2]{\sum\limits_{#1=1}^{#2}}
    In the Metropolis algorithm we calculate a ratio of determinants in the
    Metropolis test. Starting by defining a Slater determinant $D$ with entries
    defined as
        \begin{equation}
            D_{ij} \equiv \phi_j{r_i}
            \label{eq:Ddef}
        \end{equation}
    where the $\phi$'s are defined as in \Autoref{sub:vmc}. \\
    In terms of cofactors $C_{ij}$ we have
        \begin{equation}
            \det(D) = \sum\limits^N_{j=1} D_{ij}C_{ji}
            \label{eq:Dcofac}
        \end{equation}
    If we now take into light the mentioned assumption about moving only one
    particle at a time the determinant given in \Autoref{eq:Ddef} only gets a
    change of one row. \\
    Defining the ratio as
        \begin{equation}
            R \equiv \frac{\det(D\left(x^{\text{new}}\right))}
            {\det(D\left(x^{\text{old}}\right))}
            \label{eq:Rdef}
        \end{equation}
    Using the fact that when moving the particle at position $r_i$ the
    cofactors remain unchanged and inserting in \Autoref{eq:Dcofac} into
    \Autoref{eq:Rdef} we have
        \begin{equation}
            R = \frac{\sum\limits^{N}_{j=1} D_{ij}(\rnew)C_{ji}(\rold)}
            {\sum\limits^{N}_{j=1} D_{ij}(\rold)C_{ij}(\rold)}
            \label{eq:R1}
        \end{equation}
    Since the Slater is square(closed shell), we have the following ratio >>INSERT REF<<
        \begin{equation}
            \det(D) = \frac{D^{\dagger}}{D}
            \label{eq:Ddagger}
        \end{equation}
    inserting this into \Autoref{eq:R1} we get
        \begin{equation}
            R = \frac{\suml{j}{N}D_{ij}(\rnew)D^{-1}_{ji}(\rold)\det(D)(\rold)}
            {\suml{j}{N}D_{ij}(\rold)D^{-1}_{ji}(\rold)\det(D)(\rold)}
        \end{equation}
    Since $D$ is ivertible >>INSERT REF<< we have
        \begin{equation}
            \suml{k}{N}D_{ik}D^{-1}_{kj} = \delta_{ij}
            \label{eq:Dorth}
        \end{equation}
    meaning the ratio is finally(with \Autoref{eq:Ddef} inserted)
        \begin{equation}
            R = \suml{j}{N}\phi_j(\rnew_j)D^{-1}_{ji}(\rold)
        \end{equation}

\section{SETUP}
\label{sec:setup}
\section{RESULTS}
\label{sec:results}
\section{DISCUSSION}
\label{sec:discussion}
\section{CONCLUSION}
\label{sec:conclusion}

\begin{thebibliography}{}
    \bibitem{GriffQuan}
        David J. Griffiths Chapters 2, 4 and 7,
        \textit{Introduction to Quantum Mechanics Second Edition},
        Pearson, 2005, ISBN 0-13-111892-7.
    \bibitem{basicMB}
        Morten Hjort-Jensen,
        \textit{Computational Physics: Hartree-Fock methods and introduction to Many-Body Theory.} \\
        \url{https://www.github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/pub/basicMB/pdf}, 2017
    \bibitem{vmc}
        Morten Hjort-Jensen,
        \textit{Computational Physics: Variational Monte Carlo methods.} \\
        \url{https://www.github.com/CompPhysics/ComputationalPhysics2/blob/gh-pages/doc/pub/vmc/pdf}, 2017
    \bibitem{taut}
        M. Taut, 
        \textit{Two Electrons in an External Oscillator Potential; Particular analytic solutions of a Coulomb Correlation Problem},
        Physical Review A 48, 3561, 1993.
    \bibitem{NeOr}
        Negele and Orland,
        \textit{Quantum Many-Particle Systems},
        Addison-Wesley.
    \bibitem{FeWa}
        Fetter and Walecka,
        \textit{Quantum Theory of Many-Particle Systems},
        Mcgraw-Hill, 1971.
    \bibitem{DiVNe}
        Dickhoss and Van Neck,
        \textit{Many-Body Theory Exposed},
        World Scientific, 2006
\end{thebibliography}

\end{document}
